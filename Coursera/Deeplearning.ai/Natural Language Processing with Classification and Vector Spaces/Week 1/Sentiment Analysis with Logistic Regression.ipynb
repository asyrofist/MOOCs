{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised ML & Sentiment Analysis\n",
    "\n",
    "We are going to learn about supervised machine learning and specifically we are going to learn about logistic regression. In order for you to implement logistic regression, we need to take a few steps. In **supervised machine learning** you have input features $X$ and a set of labels $Y$. Now to make sure we are getting the most accurate predictions based on the data, our goal is to minimize the error rates or cost as much as possible. To do so, we are going to run your prediction function which takes in parameters data to map your features to output labels $\\hat{Y}$. Now the best mapping from features to labels is achieved when the difference between the expected values $Y$ and the predicted values $\\hat{Y}$ is minimized. The *cost function* does this by comparing how closely the output $\\hat{Y}$ is to the label $Y$. Finally, we can update the parameters and repeat the whole process until your cost is minimized, as illustred in the diagram below.\n",
    "\n",
    "<img src=\"images/supervised_ml.svg\" width=\"60%\"/>\n",
    "\n",
    "So let's take a look at the supervised machine learning classification task of sentiment analysis. In this example you have the tweet, let's say, \n",
    "\n",
    "> I'm happy because I'm learning NLP.\n",
    "\n",
    "And the objective in this task is to predict whether a tweet has a positive or a negative sentiment. We do this by starting with a training set where tweets with a positive sentiment have a label of `1`, and the tweets with a negative sentiment have a label of `0`. For this task you will use a logistic regression classifier which assigns its observations to two distinct classes. To get started building a logistic regression classifier that's capable of predicting sentiments of an arbitrary tweet, we\n",
    "- First process the raw tweets in the training set and extract useful features. \n",
    "- Then we train the logistic regression classifier while minimizing the cost. \n",
    "- Finally we are able to make predictions. Thus, given the tweet, we should classify it to either be positive or negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary & Feature Extraction\n",
    "\n",
    "In order to represent a text as a vector, we first have to build a vocabulary $V$ that will allow us to encode any text or any tweet as an array of numbers. To do so, consider a list of tweets, visually it would look like this `[tweet_1, tweet_2, ..., tweet_m]`, where `tweet_1 = \"I'm happy because I am learning NLP\"`, ..., `tweet_m = \"I hated the movie\"`. Then the vocabulary $V$ would be the list of unique words from this list of tweets. To build that list, we have to go through all the words from all the tweets and save every new word that appears in the search. In this example, we have the words \"I\", then the word, \"am\" and \"happy\", \"because\", and so forth. Note that the word \"I\" and the word \"am\" would not be repeated in the vocabulary. We would end up with a vocabulary as:\n",
    "\n",
    "```\n",
    "V = ['I', 'am', 'happy', 'because', 'learning', 'NLP', ..., 'hated', 'the', 'movie']\n",
    "```\n",
    "\n",
    "Let's take these tweets and extract features using the vocabulary. To do so, we have to check if every word from your vocabulary appears in the tweet. If it does like in the case of the word \"I\", we would assign a value of `1` to that feature. If it does not appear, we would assign a value of `0`, as shown below. \n",
    "\n",
    "<img src=\"images/feature_extraction.svg\" width=\"60%\"/>\n",
    "\n",
    "In this example above, the representation of your tweet would have six ones and many zeros. These zeros correspond to every unique word from your vocabulary that is not in the tweet. This type of representation with a small relative number of non-zero values is called a **sparse representation**. This representation would have a number of features equal to the size of your entire vocabulary $n=|V|$. This would have a lot of features equal to `0` for every tweet. With the sparse representation, a logistic regression model would have to learn $\\theta_{n+1}$ parameters, where $n$ would be equal to the size of your vocabulary. \n",
    "\n",
    "$$\n",
    "[\\theta_0, \\theta_1, \\theta_2, \\ldots, \\theta_n], \\ \\ \\ \\ \\text{where} \\ \\ \\ \\ n = |V| \n",
    "$$\n",
    "\n",
    "We can imagine that for large vocabulary sizes, this would be problematic. It would take an excessive amount of time to train your model and much more time than necessary to make predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative and Positive Frequencies\n",
    "\n",
    "We'll now learn to generates counts, which we can then use as features into your logistic regression classifier. Specifically, given a word, we want to keep track of the number of times it shows up as the positive class. Given another word we want to keep track of the number of times that word showed up in the negative class. Using both those counts, we can then extract features and use those features into the logistic regression classifier. \n",
    "\n",
    "It is helpful to first imagine how these two classes would look. For instance, imagine we have a corpus consisting of four tweets.\n",
    "\n",
    "```\n",
    "I am happy because I am learning NLP\n",
    "I am happy\n",
    "I am sad, I am not learning NLP\n",
    "I am sad\n",
    "```\n",
    "\n",
    "Associated with that corpus, we have a set of unique words, the vocabulary. In this example, the vocabulary would have eight unique words.\n",
    "\n",
    "```\n",
    "V = ['I', 'am', 'happy', 'because', 'learning', 'NLP', 'sad', 'not']\n",
    "```\n",
    "\n",
    "For this particular example of sentiment analysis, we have two classes. One class associated with **positive sentiment** (represented as label `1`) and the other with **negative sentiment** (represented as label `0`). So taking our corpus, we have a set of two tweets that belong to the positive class, and the sets of two tweets that belong to the negative class. \n",
    "\n",
    "**Positive class**\n",
    "```\n",
    "I am happy because I am learning NLP\n",
    "I am happy\n",
    "```\n",
    "\n",
    "**Negative class**\n",
    "```\n",
    "I am sad, I am not learning NLP\n",
    "I am sad\n",
    "```\n",
    "\n",
    "Let's take the sets of positive tweets and take a look at your vocabulary. To get the positive frequency (`freqPos`) in any word in your vocabulary, we have to count the times as it appears in the positive tweets. For instance, the word \"happy\" appears once in the first positive tweet, and another time in the second positive tweet. So it's positive frequency is two. The complete list looks like below. \n",
    "\n",
    "```\n",
    "V = ['I', 'am', 'happy', 'because', 'learning', 'NLP', 'sad', 'not']\n",
    "PosFreq(1) = [3, 3, 2, 1, 1, 1, 0, 0]\n",
    "```\n",
    "\n",
    "The same logic applies for getting the negative frequency (`freqNeg`). However, for the sake of clarity we look at some example, the word \"am\" appears two times in the first tweet and another time in the second one. So it's negative frequency is three. The complete list of negative frequencies are below.\n",
    "\n",
    "```\n",
    "V = ['I', 'am', 'happy', 'because', 'learning', 'NLP', 'sad', 'not']\n",
    "NegFreq(0) = [3, 3, 0, 0, 1, 1, 2, 1]\n",
    "```\n",
    "\n",
    "So this is the entire table with the positive and negative frequencies for your corpus. \n",
    "\n",
    "| Vocabulary | PosFreq(1) | NegFreq(0) |\n",
    "| :--------- | :--------: | :--------: |\n",
    "| I          |     3      |     3      |\n",
    "| am         |     3      |     3      |\n",
    "| happy      |     2      |     0      |\n",
    "| because    |     1      |     0      |\n",
    "| learning   |     1      |     1      |\n",
    "| NLP        |     1      |     1      |\n",
    "| sad        |     0      |     2      |\n",
    "| not        |     0      |     1      | \n",
    "\n",
    "In practice when coding, this table is a dictionary mapping from a (word, class) to its frequency. So it maps the word and its corresponding class to the frequency or the number of times that's where it showed up in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('I', 1): 3, ('am', 1): 3, ('happy', 1): 2, ('because', 1): 1, ('learning', 1): 1, ('NLP', 1): 1, ('I', 0): 3, ('am', 0): 3, ('sad', 0): 2, ('not', 0): 1, ('learning', 0): 1, ('NLP', 0): 1}\n"
     ]
    }
   ],
   "source": [
    "corpus = ['I am happy because I am learning NLP', 'I am happy', 'I am sad I am not learning NLP', 'I am sad']\n",
    "y = [1, 1, 0, 0]\n",
    "\n",
    "def create_vocabulary(corpus, ys):\n",
    "    \"\"\" create vocabulary as `(word, class)=freq` \"\"\"\n",
    "    vocabulary = {}\n",
    "    for sentence, y in zip(corpus, ys):\n",
    "        sent = sentence.split()\n",
    "        for word in sent:\n",
    "            pair = (word, y)\n",
    "            vocabulary[pair] = vocabulary.get(pair, 0) + 1\n",
    "    return vocabulary\n",
    "print(create_vocabulary(corpus, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>word    </td><td>freqNeg</td><td>freqPos</td></tr>\n",
       "<tr><td>I       </td><td>3      </td><td>3      </td></tr>\n",
       "<tr><td>NLP     </td><td>1      </td><td>1      </td></tr>\n",
       "<tr><td>am      </td><td>3      </td><td>3      </td></tr>\n",
       "<tr><td>because </td><td>0      </td><td>1      </td></tr>\n",
       "<tr><td>happy   </td><td>0      </td><td>2      </td></tr>\n",
       "<tr><td>learning</td><td>1      </td><td>1      </td></tr>\n",
       "<tr><td>not     </td><td>1      </td><td>0      </td></tr>\n",
       "<tr><td>sad     </td><td>2      </td><td>0      </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show dictionary\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "d = {}\n",
    "table = [('word', 'freqNeg', 'freqPos')]\n",
    "vocabulary = create_vocabulary(corpus, y)\n",
    "for word, cl in sorted(vocabulary):\n",
    "    freq = vocabulary[(word, cl)]\n",
    "    if not word in d:\n",
    "        d[word] = {0: 0, 1:0}\n",
    "    d[word][cl] = freq\n",
    "table += [(w, d[w][0], d[w][1]) for w in d]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction with Frequencies\n",
    "\n",
    "Now, we learn how to encode or represent a tweet as a vector of dimension 3. In doing so, we have a much faster speed for the logistic regression classifier, since instead of learning $|V|$ features, it only has to learn three features. \n",
    "\n",
    "We just saw that the frequency of a word in a class is simply the number of times that the word appears on the set of tweets belonging to that class and that this table is basically a dictionary mapping from (word, class) pairs, to frequencies, *i.e*, it just tells us how many times each word showed up in its corresponding class. Now that we have built the frequencies dictionary, we can use it to extract useful features for sentiment analysis. \n",
    "\n",
    "In order to represent a tweet with features, let's first look at an arbitrary tweet $m$ represented as:\n",
    "\n",
    "$$\n",
    "X_m = [1, \\sum\\limits_w \\text{freqs}(w, 1), \\sum\\limits_w \\text{freqs}(w, 0)]\n",
    "$$\n",
    "\n",
    "where $X_m$ is the set of features of tweet $m$, $1$ is the bias unit, $\\sum\\limits_w \\text{freqs}(w,1)$ is the sum of positive frequencies for every unique word on tweet $m$, and $\\sum\\limits_w \\text{freqs}(w,0)$ is the sum of negative frequencies for every unique word on tweet $m$. \n",
    "\n",
    "Thus, to extract the features for this representation, we only have to sum frequencies of words. For instance, take the following tweet.\n",
    "\n",
    "$$\n",
    "\\text{\"I am sad, I am not learning NLP\"}\n",
    "$$\n",
    "\n",
    "Now, let's look at the frequencies for the positive class from the last lecture. \n",
    "\n",
    "| Vocabulary | PosFreq(1) |\n",
    "| :--------- | :--------: |\n",
    "| I          |     3      |\n",
    "| am         |     3      |\n",
    "| happy      |     2      |\n",
    "| because    |     1      |\n",
    "| learning   |     1      |\n",
    "| NLP        |     1      |\n",
    "| sad        |     0      |\n",
    "| not        |     0      | \n",
    "\n",
    "The only words from the vocabulary that do not appear on these tweets are \"happy\" and \"because\". Now let's compute the features from the representation to the positive part of the sum. \n",
    "\n",
    "$$\n",
    "X_m[1] = \\sum\\limits_w \\text{freqs}(w, 1)\\ \\ \\ \\rightarrow\\ \\ \\ 3+3+1+1+0+0 = 8\n",
    "$$\n",
    "\n",
    "To get this value, we sum the frequencies of the words from the vocabulary that appear on the tweet. At the end, we get a value equal to eight. Now let's get the value of the third feature. It is the sum of negative frequencies of the words from the vocabulary that appear on the tweet. \n",
    "\n",
    "| Vocabulary | NegFreq(0) |\n",
    "| :--------- | :--------: |\n",
    "| I          |     3      |\n",
    "| am         |     3      |\n",
    "| happy      |     0      |\n",
    "| because    |     0      |\n",
    "| learning   |     1      |\n",
    "| NLP        |     1      |\n",
    "| sad        |     2      |\n",
    "| not        |     1      |\n",
    "\n",
    "For this example, we get 11 after summing up the underlined frequencies. \n",
    "\n",
    "$$\n",
    "X_m[2] = \\sum\\limits_w \\text{freqs}(w, 0)\\ \\ \\ \\rightarrow\\ \\ \\ 3+3+1+1+2+1 = 11\n",
    "$$\n",
    "\n",
    "\n",
    "So far, this tweets, this representation would be equal to the vector `[1, 8, 11]`. We now know how to represent a tweet as a vector of dimension 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation for Tweet m: [1, 8, 11]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = create_vocabulary(corpus, y)\n",
    "tweet = 'I am sad I am not learning NLP'\n",
    "words = list(set(tweet.split()))\n",
    "\n",
    "Xm = [1, 0, 0]\n",
    "for word in words:\n",
    "    if (word, 1) in vocabulary:\n",
    "        Xm[1] += vocabulary[(word, 1)]\n",
    "    if (word, 0) in vocabulary:\n",
    "        Xm[2] += vocabulary[(word, 0)]\n",
    "print('Vector representation for Tweet m: {}'.format(Xm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Now, we learn about two major concepts of preprocessing: **stemming** and **stop words**. Specifically, we learn how to use stemming and stop words to preprocess texts. Consider the following tweet:\n",
    "\n",
    "> @YMourri and @AndrewYNg are tuning a GREAT AI model at https://deeplearning.ai!!!\n",
    "\n",
    "The first step in preprocessing consists of removing all the words that do not add significant meaning to the tweets, *aka* **stop words** and **punctuation marks**. In practice, you would have to compare your tweet against two lists. One with stop words in English and another with punctuation. \n",
    "\n",
    "| Stop words | Punctuation |\n",
    "| :--------- | :---------: |\n",
    "| and        |      ,      |\n",
    "| is         |      .      |\n",
    "| are        |      :      |\n",
    "| at         |      !      |\n",
    "| has        |      \"      |\n",
    "| for        |      '      |\n",
    "| a          |      ;      |\n",
    "\n",
    "These lists are usually much larger, but for the purpose of this example, they will do just fine. Every word from the tweet that also appears on the list of stop words should be eliminated. So we have to eliminate the word \"and\", the word \"are\", the word \"a\", and the word \"at\". The tweet without stop words looks like this. \n",
    "\n",
    "> @YMourri @AndrewYNg tuning GREAT AI model https://deeplearning.ai!!!\n",
    "\n",
    "Note that the overall meaning of the sentence could be inferred without any effort. Now, we eliminate every punctuation mark. In this example, there are only exclamation points. The tweet without stop words and punctuation looks like this. \n",
    "\n",
    "> @YMourri @AndrewYNg tuning GREAT AI model https://deeplearning.ai\n",
    "\n",
    "However, note that in some contexts we do not eliminate punctuation. We should think carefully about whether punctuation adds important information to your specific NLP task or not. Tweets and other types of texts often have handles and URLs, but these do not add any value for the task of sentiment analysis. Let's eliminate these two handles and this URL. \n",
    "\n",
    "> tuning GREAT AI model\n",
    "\n",
    "At the end of this process, the resulting tweets contains all the important information related to its sentiment. \"Tuning GREAT AI model\" is clearly a positive tweet and a sufficiently good model should be able to classify it. Now that the tweet from the example has only the necessary information, we perform the second step in preprocessing, called **stemming**. \n",
    "\n",
    "**Stemming** in NLP is simply transforming any word to its base stem, which you could define as the set of characters that are used to construct the word and its derivatives. Let's take the first word from the example. Its stem is \"tun\", because adding the letter \"e\", it forms the word \"tune\". Adding the suffix \"ed\", forms the word \"tuned\", and adding the suffix \"ing\", it forms the word \"tuning\". After you perform stemming on your corpus, the word \"tune\", \"tuned\", and \"tuning\" will be reduced to the stem \"tun\". Hence, the vocabulary would be significantly reduced when we perform this process for every word in the corpus. \n",
    "\n",
    "To reduce your vocabulary even further without losing valuable information, we have to lowercase every one of your words. So the word \"GREAT\", \"Great\" and \"great\" would be treated as the same exact word. This is the final preprocess tweet as a list of words. \n",
    "\n",
    "```\n",
    "Tweet = ['tun', 'great', 'ai', 'model']\n",
    "```\n",
    "\n",
    "Now that we are familiar with stemming and stop words, we know the basics of texts processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it All Together\n",
    "\n",
    "You will now use everything that you learned to create a matrix that corresponds to all the features of your training example. Specifically, we walk through an algorithm that allows to generate this $X$ matrix. Previously, we saw how to preprocess a tweet like the one below to get a list of words that contain all the relevant information for the sentiment analysis tasks in NLP. \n",
    "\n",
    "> I am Happy Because I am learning NLP @deeplearning\n",
    "\n",
    "With that list of words, we are able to get a nice representation using a frequency dictionary mapping. \n",
    "\n",
    "```\n",
    "tweet = ['happy', 'learn', 'nlp']\n",
    "```\n",
    "\n",
    "Finally, we generate a vector with a bias unit and two additional features that store the sum of the number of times that every word on your process tweets appear in positive tweets and the sum of the number of times they appear in negative ones. \n",
    "\n",
    "```\n",
    "X_tweet = [1, 4, 2]\n",
    "```\n",
    "\n",
    "In practice, we perform this process on a set of $m$ tweets. So, given a set of multiple raw tweets, we preprocess them one by one to get these sets of lists of words one for each of your tweets. \n",
    "\n",
    "```\n",
    "tweet_2 = \"I am sad not learning NLP\" \n",
    "tweet_2 = ['sad', 'not', 'learn', 'nlp']\n",
    "tweet_3 = \"I am sad :(\"\n",
    "tweet_3 = ['sad']\n",
    "```\n",
    "\n",
    "Finally, we extract features using a frequencies dictionary mapping. At the end, we have a matrix, $X$ with $m$ rows and three columns where every row would contain the features for each one of your tweets. \n",
    "\n",
    "$$\n",
    "\\left [\n",
    "\\begin{matrix}\n",
    "1 & X_1^{(1)} & X_2^{(1)} \\\\\n",
    "1 & X_1^{(2)} & X_2^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & X_1^{(m)} & X_2^{(m)} \\\\\n",
    "\\end{matrix} \n",
    "\\right ] \\ \\ \\ \\ \\rightarrow \\ \\ \\ \\ \\left [\n",
    "\\begin{matrix}\n",
    "1 & 40 & 20 \\\\\n",
    "1 & 20 & 50 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & 5 & 35 \\\\\n",
    "\\end{matrix} \n",
    "\\right ]\n",
    "$$\n",
    "\n",
    "The general implementation of this process is rather easy. First, we build the frequencies dictionary, then initialize the matrix $X$ to match your number of tweets. After that, we go over through the sets of tweets carefully deleting stop words, stemming, deleting URLs, and handles and lower casing. Finally, we extract the features by summing up the positive and negative frequencies of the tweets. \n",
    "\n",
    "```python\n",
    "freqs = build_freqs(tweets, labels)           # Build frequencies dictionary\n",
    "X = np.zeros((m, 3))                          # Initialize matrix X\n",
    "\n",
    "for i in range(m):                            # For every tweet\n",
    "    p_tweet = process_tweet(tweets[i])        # Process tweet\n",
    "    X[i,:] = extract_features(p_tweet, freqs) # Extract features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Overview\n",
    "\n",
    "Now we get an overview of logistic regression. Previously, we learned to extract features, and now you will use those extracted features to predict whether a tweet has a positive sentiment or a negative sentiment. **Logistic regression** makes use of a *sigmoid* function which outputs a probability between zero and one. Let's take a look at the overview of logistic regression. In supervised machine learning, we have input features and a sets of labels. To make predictions based on data, we use a function with some parameters to map features to output labels. To get an optimum mapping from features to labels, we minimize the cost function which works by comparing how closely your output $\\hat{Y}$ is to the true labels $Y$ from data. After that the parameters are updated and we repeat the process until the cost is minimized. \n",
    "\n",
    "<img src=\"images/supervised_ml.svg\" width=\"60%\"/>\n",
    "\n",
    "For logistic regression, this *prediction function* is equal to the *sigmoid* function. The function used to classify in logistic regression $h$ is the sigmoid function and it depends on the parameters $\\theta$ and then the features vector $x^{(i)}$, where $i$ is used to denote the $i$th observation or data points. In the context of tweets, it is the $i$th tweet. \n",
    "\n",
    "$$\n",
    "h(x^{(i)}, \\theta) = \\frac{1}{1+e^{-\\theta^Tx^{(i)}}}\n",
    "$$\n",
    "\n",
    "Visually, the sigmoid function has the form below and it approaches zero as the dot product of $\\theta^Tx^{(i)}$ approaches minus infinity and one as it approaches infinity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAEvCAYAAAA0ITL9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxcdb3/8fdnZrIvTdN93xdaaGkbWjaxCrJdpHJFZFF2iiCiXhRBEBXUK7hdEVC4WlkEKgKFAkWESys/UOi+l9B0T1q6JU2zJzPz/f0x03aapk1oJzmTmdfz8ZjHnOV7Zj49PTln3nPO+Y455wQAAAAASBw+rwsAAAAAAByMoAYAAAAACYagBgAAAAAJhqAGAAAAAAmGoAYAAAAACYagBgAAAAAJJuDVG3fv3t0NHjzYq7cHAAAAAE8tWrRol3OuR0vzPAtqgwcP1sKFC716ewAAAADwlJltOtw8Ln0EAAAAgARDUAMAAACABENQAwAAAIAEQ1ADAAAAgARDUAMAAACABENQAwAAAIAEQ1ADAAAAgATTalAzsxlmtsPMVh5mvpnZg2ZWYmbLzWxi/MsEAAAAgNTRljNqj0s69wjzz5M0IvqYLun3x14WAAAAAKSuQGsNnHPvmNngIzSZJulJ55yT9L6ZFZhZH+fctjjVCAAAAByWc05hJ4XCTmEXeUSGD8zbN93tH5bC4djxA+2dotPCklOkzb52Lub9tG+eFG3jDgwrMmPfax1oF2kTKfzg149OkouOuP3/vpgFdHDbfePusPP2L9zstfa1a2W+i2176Ho/+LWaNzjyex3pfVt5qU+sW066zh7b+xhfpWO1GtTaoJ+kLTHjpdFphwQ1M5uuyFk3DRw4MA5vDQAAgHhoCoVV3xRSQ/Dg5/qmsBqCITUGw5FHKHzocCispqBTUygcfRwYbgyFFQw5BcP7niPzQmG3f3ooHJl+8HNYoWj7fcFrX/gKhZ1CzikcfW7tQz5w4oCClAxqbeace0zSY5JUVFTEnxQAAMBRCIedqhuD2lvXpL11QVXVN6mqPqiaxqCqG4KqaQiquiGkmuhwTWNItQ1B1TWFVNsYUn30ua4ppLrocyh87B/N/D5Tmt+U5vMpLeBTmt8U8EWf/T4FfKZAzDS/z5QdCESnRcYDPl/02eTzmfxm8vujzz6Tz0x+nw7M2z/N5LPIdJ9F5plp/3yfSWYHhn1mkkl+M/l80XEpOj+y7L5lLDrdTNFHZNqBZ8kUnS9JMeP7l4u2U2y7Fl5D+9vpoOHm81qabs2WUfNlYt4z1sFjB79/7Hu0NK+5Q5Zt5b2OtOwh84+49JEF/Ee/rFfiEdTKJA2IGe8fnQYAAIBW1DeFtLOqQbuqG1RR26iKmiZV1DZqT23ked+0PXVNkWBW36TqhmCbziJlp/uVkxFQbkZAWWl+Zaf7lZcZUM+8DGWn+5WV7ldWWkBZ6T5lBvzKTPMrIy0ynJHmU0bAr8w0n9IDkeGMQGQ43b9vWuQ5zR95+H2d78MwkKjiEdRmS7rFzGZKmiKpkvvTAABAqqtvCmlbZb22Vdbp48p6baus1/a99dpV3RANZo3aVdWgqoZgi8v7TCrITlfX7DR1zU5Xv4JMHdcnT/mZacrPDCg/K035mWnKiw7nZgT2h7KcDL+y0wMEJ6ATazWomdmzkqZK6m5mpZJ+KClNkpxzf5A0R9L5kkok1Uq6pr2KBQAASBR1jSFtqajV5t212lReqy3Rx9bKen1cWaeK2qZDlsnPDKhHXoa652ZoTN989cjNiI6nq3tuhgpz0tU1O/LIywzIR9ACUlZben28rJX5TtLX41YRAABAggiFnUorarV2e7XW7qhWyY5qbdpdo83ltdpR1XBQ29yMgAYUZqtvl0xNGlSgPl2y1Ds/U30KMvcPZ6X7PfqXAOhsOrQzEQAAgETknNO2ynqtLKvUhx9XqWRHJJit31mthmB4f7te+Rka0j1HU0f10MDCbA3slhN5LsxW1+y0QzpOAICjRVADAAApxTmn0oo6rSir1MqySq3culeryiq1u6Zxf5v+XbM0omeuPjWiu4b3zN3/yM9M87ByAKmEoAYAAJJaQzCkFaWVmr+xXAs2lGvx5j2qrIvcPxbwmUb0ytOZx/XU8f26aGzfLjquT56y0/mIBMBb7IUAAEBSqapv0qJNFVqwsVwLNlRoaekeNUYvXxzWI0fnHd9bJ/TvohP6ddHIXnnKTOO+MQCJh6AGAAA6Neec1myr0ryPdmjehzu1aHOFQmEnv890fN98XXnyIJ00pFBFg7qqW26G1+UCQJsQ1AAAQKezt75J767dpXnFO/TPj3Zq+95ID4xj++bra58eqlOHddeJAwqUk8FHHQCdE3svAADQKVTUNOrvqz7WK8u26oMN5QqFnfIyAzpjRA99elQPTR3ZQz3zM70uEwDigqAGAAAS1t76Jr25arteWb5V767dpWDYaWj3HN14xlB9ZnRPTRhQoIDf53WZABB3BDUAAJBQ6ptCenP1dr2ybKvmFe9UYyisfgVZuv5TQ3XBuD4a2zef3ysDkPQIagAAICGs31mtZz7YrL8tKlVlXZN65mXoipMH6vPj+2rCgALCGYCUQlADAACeaQqF9dbq7frLB5v0XsluBXymc8b21uVTBurkod3k9xHOAKQmghoAAOhw2yrr9Oz8LZo5f7N2VDWoX0GWvnP2SF1y0gD1zKNDEAAgqAEAgA6zfme1Hpm3TrOWlCnsnKaO7KH/PnmQpo7qydkzAIhBUAMAAO2u+OMqPTy3RK8u36r0gE9XnjJI1542RAMKs70uDQASEkENAAC0m5Vllfrd22v1xqrtykn3a/oZw3Td6UPUIy/D69IAIKER1AAAQNwt3bJHv33rI80t3qm8zIBuPXOErjl1sLrmpHtdGgB0CgQ1AAAQN9sq63T/6x/qpaVb1TU7Td89Z5S+esog5WemeV0aAHQqBDUAAHDM6hpDeuyd9frDP9cp5Jy+/plhumnqcOVm8FEDAI4Ge08AAHDUnHN6Zfk2/XzOGm2trNd/nNBHd5w3mk5CAOAYEdQAAMBRWbZlj+59dbUWbarQ2L75+s2XT9SUod28LgsAkgJBDQAAfCI1DUH99+tr9Jf3N6t7boYe+OI4fXFSf34HDQDiiKAGAADa7N/rduv2F5aptKJO150+RN86a4Ty6CgEAOKOoAYAAFpV2xjUA38v1uP/2qjB3bL1txtPUdHgQq/LAoCkRVADAABHtGBjub7zt2XatLtW15w2WLefM1pZ6X6vywKApEZQAwAALapvCukXbxRrxnsbNKBrtmZOP1kn01kIAHQIghoAADjEmm179fVnFmv9zhp99eRBuuO80crhN9EAoMOwxwUAAAeZtaRUd764Ql2y0vT09VN02vDuXpcEACmHoAYAACRJjcGwfvLaaj35702aMqRQv7t8gnrmZXpdFgCkJIIaAADQtso63fz0Yi3ZvEfTzxiq288ZpYDf53VZAJCyCGoAAKS4f63bpVufXaK6xpAeuWKizj+hj9clAUDKI6gBAJCinHN69J31euDvH2pI9xzNnH6yhvfM87osAIAIagAApKT6ppBue26ZXluxTeef0FsPXDxeufTqCAAJgz0yAAApprK2STc8uVDzN5brzvNGa/oZQ2VmXpcFAIhBUAMAIIVsq6zTVTPma8OuGj142QRdOL6v1yUBAFpAUAMAIEWs3V6lK2fMV1V9UE9cM1mn8vtoAJCwCGoAAKSAhRvLdd0TC5Ue8OmvN56ssX27eF0SAOAICGoAACS5N1Z9rFufXaK+BVl68trJGlCY7XVJAIBWENQAAEhiT3+wST94aaVO6F+gGVcVqVtuhtclAQDagKAGAECSenhuiX7xRrE+M6qHHr5iorLTOewDQGfBHhsAgCT06D/X6RdvFGvaiX31yy+NV5rf53VJAIBPoE17bTM718yKzazEzO5oYf5AM5trZkvMbLmZnR//UgEAQFvMeHeD/vv1D3XBuD76FSENADqlVvfcZuaX9LCk8ySNkXSZmY1p1uxuSc855yZIulTSI/EuFAAAtO6pf2/Uva+u1rlje+s3Xz5RAUIaAHRKbdl7T5ZU4pxb75xrlDRT0rRmbZyk/OhwF0lb41ciAABoi2fnb9YPXl6ls47rqQcvm8CZNADoxNpyj1o/SVtixkslTWnW5keS/mFm35CUI+msuFQHAADa5G8Lt+j7s1ZoarTjkPQAIQ0AOrN47cUvk/S4c66/pPMlPWVmh7y2mU03s4VmtnDnzp1xemsAAFLbS0vKdPsLy3X68O76w1cmKSPg97okAMAxaktQK5M0IGa8f3RarOskPSdJzrl/S8qU1L35CznnHnPOFTnninr06HF0FQMAgP1eXb5V//XcUp08pJse+2qRMtMIaQCQDNoS1BZIGmFmQ8wsXZHOQmY3a7NZ0pmSZGbHKRLUOGUGAEA7mlu8Q9+cuVRFgwr1p6uLlJVOSAOAZNFqUHPOBSXdIukNSWsU6d1xlZnda2YXRpvdJukGM1sm6VlJVzvnXHsVDQBAqltZVqmvP71Yo3vnacY1J/Fj1gCQZNq0V3fOzZE0p9m0e2KGV0s6Lb6lAQCAlpTtqdO1jy9QQVaaZlx9knIzCGkAkGzYswMA0InsrW/StX9eoLrGkJ6/6VT1ys/0uiQAQDsgqAEA0Ek0BsO66S+LtG5ntZ64drJG9c7zuiQAQDshqAEA0Ak453Tniyv0Xslu/fJL43Xa8EM6VwYAJBF+DRMAgE7gt/+3Vi8sLtW3zhqhiyf197ocAEA7I6gBAJDgnl9Uqv95a60untRf3zxzhNflAAA6AEENAIAE9u7aXbrjheU6bXg3/eyiE2RmXpcEAOgABDUAABLUxl01uunpRRrWI1e//8okpQc4bANAqmCPDwBAAqptDOrGpxbJ7zP98aoi5WemeV0SAKAD0esjAAAJxjmnO15YoY92VOmJayZrQGG21yUBADoYZ9QAAEgwM97bqNnLtuo7Z4/SGSN7eF0OAMADBDUAABLIB+t362dz1ujsMb1006eHeV0OAMAjBDUAABLEx5X1+vozSzSoMFu/umS8fD56eASAVMU9agAAJIDGYFg3Pb1ItY1BPXvDFOXReQgApDSCGgAACeC+V1dryeY9evjyiRrRK8/rcgAAHuPSRwAAPPb8olI99f4m3XjGUP3HuD5elwMASAAENQAAPLSyrFJ3zVqhU4Z203fPGeV1OQCABEFQAwDAI9UNQd3yzGIV5qTrd5dPUMDPYRkAEME9agAAeOSHL6/S5vJazZx+irrnZnhdDgAggfDVHQAAHnh5aZleWFyqWz4zXJOHFHpdDgAgwRDUAADoYFvKa3X3rJWaNKirbj1zhNflAAASEEENAIAO1BQK69aZSyST/ufLJ3JfGgCgRdyjBgBAB/rtW2u1ZPMe/e6yCRpQmO11OQCABMXXeAAAdJB/r9uth+eV6EuT+uvz4/t6XQ4AIIER1AAA6AAVNY369l+Xaki3HP3owrFelwMASHAENQAA2plzTt97Ybl21zTowcsmKCeDOw8AAEdGUAMAoJ09/cFm/WP1dt1+zmgd36+L1+UAADoBghoAAO2oZEeV7nt1tT41oruuO32I1+UAADoJghoAAO0kGArrtueWKTvdr19dMl4+n3ldEgCgk+AieQAA2smj76zXstJKPXT5BPXMy/S6HABAJ8IZNQAA2sGabXv1P299pP8Y10cXjKMrfgDAJ0NQAwAgzpqilzx2yUrTfdOO97ocAEAnxKWPAADE2UNvl2j1tr169KuTVJiT7nU5AIBOiDNqAADE0cqySj08t0QXTeinc8b29rocAEAnRVADACBOGoIh3fbcMhXmpOtHnx/rdTkAgE6MSx8BAIiT3761VsXbq/Tnq09Sl+w0r8sBAHRinFEDACAOlmyu0B/+uU6XFPXXZ0b39LocAEAnR1ADAOAY1TeF9J2/LVPv/EzdfcEYr8sBACQBLn0EAOAY/eofxVq3s0ZPXTdZ+Zlc8ggAOHacUQMA4Bgs2VyhP767QZdPGahPjejhdTkAgCRBUAMA4Cg1BsO644UV6p2fqTvPG+11OQCAJNKmoGZm55pZsZmVmNkdh2lziZmtNrNVZvZMfMsEACDxPPrPdSreXqWffOF45XHJIwAgjlq9R83M/JIelvQ5SaWSFpjZbOfc6pg2IyTdKek051yFmdHdFQAgqZXsqNLv3i7RBeP66MzjenldDgAgybTljNpkSSXOufXOuUZJMyVNa9bmBkkPO+cqJMk5tyO+ZQIAkDjCYac7XlihrHS/fsgPWwMA2kFbglo/SVtixkuj02KNlDTSzN4zs/fN7Nx4FQgAQKJ5ev5mLdxUoR9cMEY98jK8LgcAkITi1T1/QNIISVMl9Zf0jpmd4JzbE9vIzKZLmi5JAwcOjNNbAwDQcbZV1un+1z/U6cO764sTm39vCQBAfLTljFqZpAEx4/2j02KVSprtnGtyzm2Q9JEiwe0gzrnHnHNFzrmiHj3owhgA0Lk453T3rJUKhsP62UUnyMy8LgkAkKTaEtQWSBphZkPMLF3SpZJmN2vzkiJn02Rm3RW5FHJ9HOsEAMBzr63Ypv/7cIdu+9woDeyW7XU5AIAk1mpQc84FJd0i6Q1JayQ955xbZWb3mtmF0WZvSNptZqslzZX0Xefc7vYqGgCAjlZR06gfzV6lcf276JrTBntdDgAgybXpHjXn3BxJc5pNuydm2En6r+gDAICk89M5a1RR26Qnr52igL9NP0MKAMBR40gDAEAr3l27S88vKtWNZwzVmL75XpcDAEgBBDUAAI6gvimk789aoSHdc3TrmYf0kwUAQLuIV/f8AAAkpYfeLtHm8lo9c8MUZab5vS4HAJAiOKMGAMBhrN1epUffWaf/nNhPpw7r7nU5AIAUQlADAKAF4bDTXbNWKicjoLvOP87rcgAAKYagBgBAC55fVKr5G8v1/fOOU7fcDK/LAQCkGIIaAADN7K5u0M9eX6PJgwv1paL+XpcDAEhBBDUAAJr56Zw1qmkI6qcXHS8z87ocAEAKIqgBABDjXyW79OLiMt14xjCN6JXndTkAgBRFUAMAIKohGNLdL63UoG7ZuuWzw70uBwCQwvgdNQAAon4/b53W76rRk9dO5jfTAACe4owaAACS1u2s1iNz1+nC8X11xsgeXpcDAEhxBDUAQMpzzunuWSuVmebT3Rfwm2kAAO8R1AAAKW/WkjL9e/1ufe+80eqZl+l1OQAAENQAAKltT22jfvraGk0YWKDLThrodTkAAEiiMxEAQIq7/+/F2lPXpKe+cIJ8Pn4zDQCQGDijBgBIWYs2VejZ+Zt1zamDNaZvvtflAACwH0ENAJCSgqGw7pq1Qn26ZOpbnxvpdTkAAByEoAYASEmP/2ujPvy4Sj/8/FjlZnAnAAAgsRDUAAApZ+ueOv36zY/02dE9dc7YXl6XAwDAIQhqAICU8+NXVinsnH584ViZ0YEIACDxENQAACnl/9Zs1xurtuvWM0doQGG21+UAANAighoAIGXUNYZ0z8urNKJnrq4/fajX5QAAcFjcPQ0ASBkPvr1WZXvq9NfpJys9wHeVAIDExVEKAJASPtpepf99Z70untRfU4Z287ocAACOiKAGAEh64bDT3bNWKjczoDvPG+11OQAAtIqgBgBIes8vKtX8jeW649zR6pab4XU5AAC0iqAGAEhqu6sb9LPX1+ikwV11SdEAr8sBAKBNCGoAgKT20zlrVF0f1E8vOkE+H7+ZBgDoHAhqAICk9a91u/Ti4jLd+OmhGtkrz+tyAABoM4IaACApNQRDunvWSg0szNY3PjvC63IAAPhE+B01AEBS+v28dVq/q0ZPXDtZmWl+r8sBAOAT4YwaACDprN9ZrUfmrtPnx/fVp0f28LocAAA+MYIaACCpOOd090srlZHm0w8uOM7rcgAAOCoENQBAUpm1pEz/Wrdb3zt3tHrmZXpdDgAAR4WgBgBIGntqG/XT19ZowsACXT55oNflAABw1OhMBACQNH7++ofaU9ekv/CbaQCATo4zagCApDB/Q7lmLtii608fouP65HtdDgAAx4SgBgDo9BqCIX1/1gr1K8jSN8/iN9MAAJ0flz4CADq9R+auU8mOaj1+zUnKTufQBgDo/DijBgDo1D7aXqVH5pXoogn9NHVUT6/LAQAgLtoU1MzsXDMrNrMSM7vjCO2+aGbOzIriVyIAAC0LhZ1uf3658jLT9IMLxnhdDgAAcdNqUDMzv6SHJZ0naYyky8zskKOhmeVJ+qakD+JdJAAALXnq3xu1dMse3XPBGBXmpHtdDgAAcdOWM2qTJZU459Y75xolzZQ0rYV290m6X1J9HOsDAKBFpRW1euCNYk0d1UPTTuzrdTkAAMRVW4JaP0lbYsZLo9P2M7OJkgY4516LY20AALTIOae7X1opSfrJF46XGb+ZBgBILsfcmYiZ+ST9WtJtbWg73cwWmtnCnTt3HutbAwBS1OxlWzWveKe+e84o9e+a7XU5AADEXVuCWpmkATHj/aPT9smTdLykeWa2UdLJkma31KGIc+4x51yRc66oR48eR181ACBlldc06sevrNaJAwp05SmDvS4HAIB20ZagtkDSCDMbYmbpki6VNHvfTOdcpXOuu3NusHNusKT3JV3onFvYLhUDAFLafa+uVlV9k+7/4jj5fVzyCABITq0GNedcUNItkt6QtEbSc865VWZ2r5ld2N4FAgCwz7ziHZq1pEw3TR2uUb3zvC4HAIB2E2hLI+fcHElzmk275zBtpx57WQAAHKymIai7Zq3UsB45+vpnhnldDgAA7apNQQ0AAK/d//cPVbanTs9/7RRlBPxelwMAQLs65l4fAQBob++V7NKT/96ka04brKLBhV6XAwBAuyOoAQASWlV9k25/frmGdM/R7eeM9rocAAA6BJc+AgAS2s/mrNG2yjr97WunKiudSx4BAKmBM2oAgIQ1r3iHnp2/RTecMVSTBnX1uhwAADoMQQ0AkJAq65p0xwsrNKJnrr591kivywEAoENx6SMAICHd+8pq7axu0GNXTlJmGpc8AgBSC2fUAAAJ583V2/XC4lLdPHWYxvUv8LocAAA6HEENAJBQKmoadeeLKzS6d56+8dkRXpcDAIAnuPQRAJBQ7pm9SntqG/XEtScpPcD3iQCA1MQREACQMOas2KZXlm3VrWeO0Ni+XbwuBwAAzxDUAAAJYUdVve5+aaVO6NdFN00d5nU5AAB4iqAGAPBcOOx023PLVNMQ1K8vGa80P4cnAEBq40gIAPDcn97doP+3dpd+cMEYjeiV53U5AAB4jqAGAPDUitJKPfDGhzp7TC9dMWWg1+UAAJAQCGoAAM/UNAR168wl6paTofu/OE5m5nVJAAAkBLrnBwB45sevrNLG3TV65vqT1TUn3etyAABIGJxRAwB44tXlW/XcwlJ9fepwnTKsm9flAACQUAhqAIAOt6W8Vne+uEITBhbom2eN8LocAAASDkENANChgqGwvvXXpZKTHrx0Al3xAwDQAu5RAwB0qN+9XaJFmyr020tP1IDCbK/LAQAgIfE1JgCgw8zfUK7fvb1W/zmxn6ad2M/rcgAASFgENQBAh9hZ1aBbn12iAYXZunfa8V6XAwBAQuPSRwBAuwuGwrrlmcXaU9eoF68+TbkZHH4AADgSjpQAgHb389c/1AcbyvWbL4/XmL75XpcDAEDC49JHAEC7emXZVv3x3Q26+tTBumhCf6/LAQCgUyCoAQDaTfHHVbr9+eUqGtRV3z//OK/LAQCg0yCoAQDaRWVdk258aqFyMwN65IqJSg9wyAEAoK04agIA4i4cdrrtuaUqrajT76+YqJ75mV6XBABAp0JQAwDE3UNzS/TWmh36wQVjVDS40OtyAADodAhqAIC4mlu8Q7956yNdNKGfrjxlkNflAADQKRHUAABxs3l3rb757BKN7p2vn110gszM65IAAOiUCGoAgLjYU9uoqx+fLzPTo1+ZpKx0v9clAQDQaRHUAADHrCEY0vSnFqm0vE7/e2WRBnbL9rokAAA6tYDXBQAAOrdw2Om7f1uu+RvK9eBlEzR5CJ2HAABwrDijBgA4Jr/8R7FmL9uq288dpQvH9/W6HAAAkgJBDQBw1J75YLMembdOl00eqJs+PczrcgAASBoENQDAUZlbvEM/eHmlpo7qofumjaWHRwAA4oigBgD4xFZtrdQtTy/W6N55eujyiQr4OZwAABBPHFkBAJ/I1j11uvbxBeqSlaYZV5+k3Az6pQIAIN7aFNTM7FwzKzazEjO7o4X5/2Vmq81suZn9n5kNin+pAACvVdY26drHF6i2IaQ/XzNZvfIzvS4JAICk1GpQMzO/pIclnSdpjKTLzGxMs2ZLJBU558ZJel7SA/EuFADgrb31Tbpyxgdav7NGf/jqJI3qned1SQAAJK22nFGbLKnEObfeOdcoaaakabENnHNznXO10dH3JfWPb5kAAC9VNwR11Yz5Wr1tr37/lYk6bXh3r0sCACCptSWo9ZO0JWa8NDrtcK6T9PqxFAUASBw1DUFd8+f5WlFaqYcun6gzj+vldUkAACS9uN4BbmZfkVQk6dOHmT9d0nRJGjhwYDzfGgDQDuoaQ7ruiQVavHmPHrx0gs4Z29vrkgAASAltOaNWJmlAzHj/6LSDmNlZku6SdKFzrqGlF3LOPeacK3LOFfXo0eNo6gUAdJD6ppBueHKh5m8o168vGa//GNfH65IAAEgZbQlqCySNMLMhZpYu6VJJs2MbmNkESY8qEtJ2xL9MAEBHqm8K6canFum9dbv0i4vHa9qJR7riHQAAxFurQc05F5R0i6Q3JK2R9JxzbpWZ3WtmF0ab/UJSrqS/mdlSM5t9mJcDACS4xmBYNz+9WP/8aKfu/89x+uIk+ocCAKCjtekeNefcHElzmk27J2b4rDjXBQDwQH1TSLc8s0Rvf7hDP73oeF1y0oDWFwIAAHEX185EAACd157aRl3/xEIt2lyh+6aN1RVTBnldEgAAKYugBgDQ1j11umrGfG3aXauHLptIxyEAAHiMoAYAKe6j7VW68k/zVdMQ1OPXnqRTh/Fj1gAAeEO4F/gAABFfSURBVI2gBgApbP6Gcl3/xAJlpvn11xtP0Zi++V6XBAAARFADgJT195Uf69aZS9S/a5aeuGayBhRme10SAACIIqgBQAr6y/ubdM/LKzWuf4FmXH2SCnPSvS4JAADEIKgBQAoJhsJ64I1iPfbOen12dE89dPkEZadzKAAAINFwdAaAFLGrukG3PLNY768v11dOHqgffX6sAn6f12UBAIAWENQAIAUs3lyhm/+yWBW1jfrll8br4kn9vS4JAAAcAUENAJKYc05/eX+T7n11tXp3ydSLN5+qsX27eF0WAABoBUENAJJUXWNId81aoReXlOmzo3vqN5ecqC7ZaV6XBQAA2oCgBgBJaNPuGt341CIVb6/St88aqW98drh8PvO6LAAA0EYENQBIIs45vbS0TPe8vEo+M824+iR9ZlRPr8sCAACfEEENAJLEzqoGfX/WCr25ersmDizQby+dwI9YAwDQSRHUAKCTc87p1eXbdM/LK1XTGNJd5x+na08fIj+XOgIA0GkR1ACgE9td3aAfvLxSc1Z8rPEDCvSrL43X8J65XpcFAACOEUENADqp11ds090vrVRVfVDfO3e0bvjUEH7AGgCAJEFQA4BOpmxPnX722hq9tmKbTujXRb+6ZLxG9srzuiwAABBHBDUA6CRqG4P6wz/X67F31sk56bbPjdTXpg5TGmfRAABIOgQ1AEhw4bDTy8vKdP/rxfp4b70uGNdHd5w3Wv270qMjAADJiqAGAAlsyeYK/fiV1Vq6ZY9O6NdFD10+QUWDC70uCwAAtDOCGgAkoLI9dfrlG8WataRMPfIy9IuLx+mLE/vLR5f7AACkBIIaACSQTbtr9MjcdXphcal8PtPNU4fp5s8MV24Gu2sAAFIJR34ASAAlO6r08Nx1enlpmQJ+n66YMlDTPz1M/QqyvC4NAAB4gKAGAB5avXWvHpq7Vq+v/FiZAb+uO32IbvjUUPXMz/S6NAAA4CGCGgB0MOec/rVut/783ga9tWaHcjMCunnqMF13+lAV5qR7XR4AAEgABDUA6CB7ahv1/KJSPfPBZq3fVaOu2Wn69lkjdfWpg9UlO83r8gAAQAIhqAFAO3LOaemWPfrL+5v16vKtagiGNXFggX59yXidf0IfZab5vS4RAAAkIIIaALSD8ppGzVmxTc/O36xVW/cqJ92viyf11xVTBmlM33yvywMAAAmOoAYAcbK3vkn/WLVdryzbqndLdikUdhrdO0/3feF4XTShH13sAwCANuNTAwAcg9rGoN5as0OvLNuqfxbvVGMorH4FWbrhU0P1+fF9NKZPvsz4kWoAAPDJENQA4BPaVlmnecU7Na94h975aJfqmkLqlZ+hr5w8SJ8f30cnDiggnAEAgGNCUAOAVjSFwlq4sULzindoXvFOFW+vkiT1K8jSxZP664JxfXTS4EL5fIQzAAAQHwQ1AGgmFHZas22vFmws1/vrd+u9kt2qbggqzW86aXCh7pp0nKaO6qHhPXM5cwYAANoFQQ1AyqtvCmnplj1auLFc8zdWaPGmClU3BCVFzppdeGJfTR3ZQ6cO706HIAAAoEPwiQNASmkKhbV2e7VWbq3UqrJKLS+r1KqyvWoMhSVJo3rladqJfTV5SKFOGlyovgVZHlcMAABSEUENQNKqqm/Sup01WrNtr1aURYLZmo+r1BiMhLKcdL/G9M3XNacN1kmDC1U0uKsKstM9rhoAAICgBiAJVNQ0au2OapXsqNbaHVUqiQ5vq6zf3yYvM6Dj+3bRVacM0vH9uuj4fl00pFsOHYAAAICERFADkPCCobC2VdZrc3mtNpfXatPuWm0pr9Wm8hpt3l2rvfXB/W2z0vwa3jNXpwztpmE9czWiZ65G9c7TwMJsOv4AAACdBkENgKfqm0LaVd2g7Xsb9HFlvbZV1kWfDwxvr2pQKOz2L5PmN/Xvmq2BhdmaMKCrBnXL3h/K+nbJ4iwZAADo9AhqAOKqvimkitpGVdQ0aU9toypqm6LjjdpZ3aBd1Q3aVdWoXdUN2lndoKqYs2H7ZKb51LdLlvoUZOqUYd3Vp0umBhRmaUBhtgZ1y1Hv/Ez5CWMAACCJtSmomdm5kn4ryS/pj865nzebnyHpSUmTJO2W9GXn3Mb4lgqgPYXDTnVNIdU0BFXdEFRNQyj6HFRNY2RadX1QVfVB7a1v0t66ppjhoKrqm1RR26S6ptBh3yM/M6DueRnqnpuh4/rm64zcDHXPTVf33Az1zM9Qny5Z6tslS/lZAS5TBAAAKa3VoGZmfkkPS/qcpFJJC8xstnNudUyz6yRVOOeGm9mlku6X9OX2KBhINs45BcNOobBTUyisYMipKRxWU8ipKRhWUyisxlB0PBRWUzCshlBYjcGYR8x4QzCk+qYDz/VNITUEI8/1wbDqG0OqawqptjGo+qawahuDqm2MtGkLn0n5WWnKywwoPzNN+ZlpGtw9W3mZaSrISlPXnHR1zU5X1+w0FWSnq2tOmrpmp6sgO00ZAX87r00AAIDk0JYzapMllTjn1kuSmc2UNE1SbFCbJulH0eHnJT1kZuacc+pEVpZVauueuqNevrV/7KFrwx1h3pFfu3l716xF7Hx30PRD3+jgtu7Qae7g19n3Gi5mYuxyLmY5J3fw8s4dPD86vm+5/a8d87ph18JruUOnx75WODocjr5wONo+7NyB+funO4XDB9q46LTQvvnh6Hj4QPtQOOY5LAXD4Uj78IF5+8JXMBxWKOQUcvvGXSSMhcL7x+PNTMoM+JWZ5lNmml8ZgQPPWel+dc9NV3Z6tjLT/MpO9ysr3a+stMhzTkZAuRl+5aQHlJsRUE70kZsRUG5mQDnpfs52AQAAtLO2BLV+krbEjJdKmnK4Ns65oJlVSuomaVdsIzObLmm6JA0cOPAoS24/M97boBcXl3ldBo6SmWSSzEw+k0wmM8m3b9wOjO9r6/eZzEz+mDY+X2RZvy8yzWd2oJ1P8tu+4ehyPik7EJDPZ/KbosuZ0vy+SJvoI3DQsE9pflPAb/L7fErzmQJ+nwK+yLQ0v0/pfp/SApHhgM+n9Ohwmt+n9EBkfkYgOhwdTw/4lBHwK81vhCkAAIBOrEM7E3HOPSbpMUkqKipKuLNt3z5rpK49bcgxvUZrn41NBzeIbX8sy0bmH6kWa3G67Z9mLUw79L0PeU87sGzscrHtLWbmvvAU+74Wu4wdHLgkHRS69r2mb/9yBwIWAAAAkCzaEtTKJA2IGe8fndZSm1IzC0jqokinIp3KgMLsg/6hAAAAAOAFXxvaLJA0wsyGmFm6pEslzW7WZrakq6LDF0t6u7PdnwYAAAAAiaLVM2rRe85ukfSGIt3zz3DOrTKzeyUtdM7NlvQnSU+ZWYmkckXCHAAAAADgKLTpHjXn3BxJc5pNuydmuF7Sl+JbGgAAAACkprZc+ggAAAAA6EAENQAAAABIMAQ1AAAAAEgwBDUAAAAASDAENQAAAABIMAQ1AAAAAEgwBDUAAAAASDDmnPPmjc12StrkyZsfWXdJu7wuIkWx7r3DuvcO695brH/vsO69w7r3DuveO4m67gc553q0NMOzoJaozGyhc67I6zpSEeveO6x777DuvcX69w7r3juse++w7r3TGdc9lz4CAAAAQIIhqAEAAABAgiGoHeoxrwtIYax777DuvcO69xbr3zuse++w7r3DuvdOp1v33KMGAAAAAAmGM2oAAAAAkGBSMqiZ2ZfMbJWZhc2sqNm8O82sxMyKzeycwyw/xMw+iLb7q5mld0zlySW67pZGHxvNbOlh2m00sxXRdgs7us5kZGY/MrOymPV//mHanRv9Wygxszs6us5kZGa/MLMPzWy5mc0ys4LDtGO7j5PWtmMzy4juj0qi+/bBHV9l8jGzAWY218xWR4+532yhzVQzq4zZF93jRa3JqrX9iEU8GN32l5vZRC/qTDZmNipmm15qZnvN7FvN2rDtx4mZzTCzHWa2MmZaoZm9aWZro89dD7PsVdE2a83sqo6rum1S8tJHMztOUljSo5K+45xbGJ0+RtKzkiZL6ivpLUkjnXOhZss/J+lF59xMM/uDpGXOud935L8h2ZjZryRVOufubWHeRklFzrlE/O2LTsnMfiSp2jn3yyO08Uv6SNLnJJVKWiDpMufc6g4pMkmZ2dmS3nbOBc3sfklyzn2vhXYbxXZ/zNqyHZvZzZLGOee+ZmaXSrrIOfdlTwpOImbWR1If59xiM8uTtEjSF5qt+6mKHIcv8KjMpNbafiT6Jd03JJ0vaYqk3zrnpnRchckvug8qkzTFObcpZvpUse3HhZmdIala0pPOueOj0x6QVO6c+3n0C7quzY+1ZlYoaaGkIklOkX3UJOdcRYf+A44gJc+oOefWOOeKW5g1TdJM51yDc26DpBJFQtt+ZmaSPivp+eikJyR9oT3rTXbRdXqJIiEZiWOypBLn3HrnXKOkmYr8jeAYOOf+4ZwLRkffl9Tfy3pSQFu242mK7MulyL79zOh+CcfAObfNObc4OlwlaY2kft5WhWamKfLh1jnn3pdUEA3YiJ8zJa2LDWmIL+fcO5LKm02O3a8f7rP6OZLedM6VR8PZm5LObbdCj0JKBrUj6CdpS8x4qQ49qHSTtCfmg1ZLbfDJfErSdufc2sPMd5L+YWaLzGx6B9aV7G6JXuoy4zCXBLTl7wHH5lpJrx9mHtt9fLRlO97fJrpvr1RkX484iV5OOkHSBy3MPsXMlpnZ62Y2tkMLS36t7UfYz7e/S3X4L6LZ9ttPL+fctujwx5J6tdAm4bf/gNcFtBcze0tS7xZm3eWce7mj60lVbfx/uExHPpt2unOuzMx6SnrTzD6MfnuCIzjSupf0e0n3KXIQv0/SrxQJDYiDtmz3ZnaXpKCkpw/zMmz3SApmlivpBUnfcs7tbTZ7saRBzrnq6GV4L0ka0dE1JjH2Ix6ySB8GF0q6s4XZbPsdxDnnzKxT3uuVtEHNOXfWUSxWJmlAzHj/6LRYuxW5NCAQ/ea1pTaIau3/wcwCkv5T0qQjvEZZ9HmHmc1S5FImDjStaOvfgJn9r6RXW5jVlr8HtKAN2/3Vki6QdKY7zI3CbPdx05bteF+b0ug+qYsi+3ocIzNLUySkPe2ce7H5/Njg5pybY2aPmFl37s2MjzbsR9jPt6/zJC12zm1vPoNtv91tN7M+zrlt0ct5d7TQpkzS1Jjx/pLmdUBtbcaljwebLelSi/QANkSRbzbmxzaIfqiaK+ni6KSrJHGG7uidJelD51xpSzPNLCd6E7rMLEfS2ZJWttQWbdfsHoSL1PI6XSBphEV6OU1X5PKN2R1RXzIzs3Ml3S7pQudc7WHasN3HT1u249mK7MulyL797cMFaLRd9D6/P0la45z79WHa9N53P6CZTVbkcwkhOQ7auB+ZLelKizhZkU69tgnxctgrhtj2213sfv1wn9XfkHS2mXWN3gJydnRawkjaM2pHYmYXSfqdpB6SXjOzpc65c5xzq6I9Oq5W5JKkr+/r8dHM5ki63jm3VdL3JM00s59IWqLIgQhH55Brt82sr6Q/OufOV+Sa4lnRfVlA0jPOub93eJXJ5wEzO1GRSx83SrpROnjdR3slvEWRnZZf0gzn3CqvCk4iD0nKUOQyJEl6P9rbINt9Ozjcdmxm90pa6Jybrcg+/CkzK1HkhvRLvas4qZwm6auSVtiBn1/5vqSBkuSc+4MiwfgmMwtKqpN0KSE5blrcj5jZ16T963+OIj0+lkiqlXSNR7UmnWg4/pyix9fotNh1z7YfJ2b2rCJnxrqbWamkH0r6uaTnzOw6SZsU6bROFvlZrq855653zpWb2X2KfKEnSfc655p3SuKplOyeHwAAAAASGZc+AgAAAECCIagBAAAAQIIhqAEAAABAgiGoAQAAAECCIagBAAAAQIIhqAEAAABAgiGoAQAAAECCIagBAAAAQIL5/yrGSYo0YQHLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import utils\n",
    "utils.plot_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, a threshold is needed. Usually, it is set to be 0.5 and this value corresponds to a dot product between $\\theta^Tx^{(i)} = 0$. So whenever the dot product is greater or equal than zero, the prediction is positive, and whenever the dot product is less than zero, the prediction is negative. \n",
    "\n",
    "$$\n",
    "\\text{sentiment} = \\left\\{\\begin{matrix}\n",
    "\\color{green}{\\text{positive}} & \\theta^Tx^{(i)} \\ge 0\\\\ \n",
    "\\color{red}{\\text{negative}} & \\theta^Tx^{(i)} \\lt 0\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "So let's look at an example in the context of tweets and sentiment analysis. Look at the following tweet. \n",
    "\n",
    "> @YMourri @AndrewYNg tuning GREAT AI model https://deeplearning.ai!!!\n",
    "\n",
    "After a preprocessing, we end up with a list like this. \n",
    "\n",
    "```\n",
    "Tweet = ['tun', 'great', 'ai', 'model']\n",
    "```\n",
    "\n",
    "Note that some elements are deleted, everything is in lowercase and the word tuning is reduced to its stem \"tun\". Then we are able to extract features given a frequencies dictionary and arrive at a vector similar to the following. Consider using a bias units as below, and two features that are the sum of positive and negative frequencies of all the words in your processed tweets. \n",
    "\n",
    "$$\n",
    "x^{(i)} =\n",
    "\\left [\n",
    "\\begin{matrix}\n",
    "1  \\\\\n",
    "3476 \\\\\n",
    "245 \\\\\n",
    "\\end{matrix} \n",
    "\\right ] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\theta = \\left [\n",
    "\\begin{matrix}\n",
    "0.00003 \\\\\n",
    "0.00150 \\\\\n",
    "-0.00120 \\\\\n",
    "\\end{matrix} \n",
    "\\right ]\n",
    "$$\n",
    "\n",
    "Now assuming that we have an optimum sets of parameters $\\theta$, we are able to get the value of the sigmoid function, in this case, equal to 4.92, and finally, predict a positive sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "θx⁽ᶦ⁾ = 4.920030000000001\n",
      "Sigmoid = 0.9927539762129266\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(X, theta):\n",
    "    return 1./(1+np.exp(-np.dot(theta.T, X)))\n",
    "\n",
    "X = np.array([1, 3476, 245])\n",
    "theta = np.array([0.00003, 0.00150, -0.00120])\n",
    "print('\\u03B8x⁽ᶦ⁾ = {}'.format(np.dot(theta.T, X)))\n",
    "print('Sigmoid = {}'.format(sigmoid(X, theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: Training\n",
    "\n",
    "In the previous section, we learned how to classify whether a tweet has a positive sentiment or negative sentiment, using a theta that was given to us. Now, we learn theta from scratch. \n",
    "\n",
    "To train a logistic regression classifier, iterate until finding the set of parameters $\\theta$ that minimizes the cost function. Let us suppose that the loss only depends on the parameters $\\theta_1$ and $\\theta_2$. We would have a cost function that looks like the contour plot shown on the left of the image below. On the right, we can see the evolution of the cost function as we iterate. First, we initialize the parameters $\\theta$. Then, we update the $\\theta$ values in the direction of the gradient of the cost function. After many iterations, we derive to a point near to the optimum costs, and thus we can end the training. \n",
    "\n",
    "<img src=\"images/train_rl.svg\" width=\"70%\"/>\n",
    "\n",
    "In order to train the classifier, we perform the following pipeline illustred below. First, we initialize the parameter vector $\\theta$. Then we use the logistic function to get values for each of the observations. After, we are able to calculate the gradients of the cost function and update the parameters. Finally, we compute the cost $J$ and determine if more iterations are needed according to a stop-parameter or maximum number of iterations. This algorithm is known as **gradient descent**. Now, that we have the $\\theta$ variable, we evaluate it, meaning we evaluate the classifier. \n",
    "\n",
    "<img src=\"images/gradient_descent.svg\" width=\"90%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Cost: 0.8410897842728332\n",
      "Epoch 2 - Cost: 0.822010091533894\n",
      "Epoch 3 - Cost: 0.8034467007140831\n",
      "Epoch 4 - Cost: 0.7854018751562352\n",
      "Epoch 5 - Cost: 0.7678763827613204\n",
      "Epoch 6 - Cost: 0.7508694539886824\n",
      "Epoch 7 - Cost: 0.734378766722554\n",
      "Epoch 8 - Cost: 0.7184004609890954\n",
      "Epoch 9 - Cost: 0.702929184765808\n",
      "Epoch 10 - Cost: 0.6879581701026727\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(X, theta):\n",
    "    return 1. / (1 + np.exp(-np.dot(X, theta)))\n",
    "    \n",
    "def training(X, Y, lr=1e-8, epochs=10):\n",
    "    theta = np.array([1, np.random.rand()*5e-3, np.random.rand()*1e-4]).reshape(3, 1)\n",
    "    for e in range(epochs):\n",
    "        h = sigmoid(X, theta)\n",
    "        J = -1./X.shape[0] * (np.dot(Y.T, np.log(h)) + np.dot((1-Y).T, np.log(1 - h)))\n",
    "        theta = theta - lr/X.shape[0] * (np.dot(X.T, h-Y))\n",
    "        print('Epoch {} - Cost: {}'.format(e+1, J.squeeze()))\n",
    "        \n",
    "X, Y = utils.load_examples()\n",
    "training(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: Testing\n",
    "\n",
    "Now that we have data, we use this data to predict new data points. For example, given a new tweet, we use this data to say whether this tweet is positive or negative. In doing so, we analyze whether the model generalizes well or not. \n",
    "\n",
    "In order to do this, we need $X_{val}$ and $Y_{val}$ (the validation sets), and $\\theta$ (the optimum parameters from training set). Thus, we compute the sigmoid function for $X_{val}$ with parameters $\\theta$, and evaluate if each value of $h(X_{val}\\theta)$ is greater than or equal to a threshold value (often set to 0.5, thus, $\\text{pred}_{pos} = h(X_{val}\\theta) \\ge 0.5$). For example, consider the vector, 0.3, 0.8, 0.5, etc., up to the number of examples from the validation set, we assert if each of its components is greater than or equal to 0.5. If so, we assign a value of one to the resulting vector, and zero otherwise. At the end, we have a vector populated with zeros and ones indicating predicted negative and positive examples, respectively, as show below. \n",
    "\n",
    "$$\n",
    "x^{(i)} =\n",
    "\\left [\n",
    "\\begin{matrix}\n",
    "0.3  \\\\\n",
    "0.8 \\\\\n",
    "0.5 \\\\\n",
    "\\ldots \\\\\n",
    "h_m\n",
    "\\end{matrix} \n",
    "\\right ] \\ge 0.5 = \\left [\n",
    "\\begin{matrix}\n",
    "0.3 \\ge 0.5  \\\\\n",
    "0.8 \\ge 0.5 \\\\\n",
    "0.5 \\ge 0.5 \\\\\n",
    "\\ldots \\\\\n",
    "\\text{pred}_m \\ge 0.5\n",
    "\\end{matrix} \n",
    "\\right ] = \\left [\n",
    "\\begin{matrix}\n",
    "0  \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\ldots \\\\\n",
    "\\text{pred}_m\n",
    "\\end{matrix} \n",
    "\\right ]\n",
    "$$\n",
    "\n",
    "\n",
    "After building the predictions vector, we can compute the accuracy of the model over the validation sets. To do so, we compare the predictions with the true value for each observation from the validation data. If the values are equal and the prediction is correct, we get a value of 1 and 0 otherwise. For instance, consider the vectors below, if the prediction is correct, like in the case where the prediction and the label are both equal to 0, the resulting vector will set a value equal to 1 in the first position. Conversely, if the second prediction is not correct, because the prediction and label disagree, the vector will set a value of 0 in the second position, and so on.\n",
    "\n",
    "$$\n",
    "\\left [\n",
    "\\begin{matrix}\n",
    "0  \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\ldots \\\\\n",
    "\\text{pred}_m\n",
    "\\end{matrix} \n",
    "\\right ] == \\left [\n",
    "\\begin{matrix}\n",
    "0  \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\ldots \\\\\n",
    "Y_{val_m}\n",
    "\\end{matrix} \n",
    "\\right ] = \\left [\n",
    "\\begin{matrix}\n",
    "1  \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\ldots \\\\\n",
    "\\text{pred}_m == Y_{val_{m}}\n",
    "\\end{matrix} \n",
    "\\right ]\n",
    "$$\n",
    "\n",
    "After having compared the values of every prediction with the true labels of the validation set, we can get the total times that the predictions were correct by summing up the vector of the comparisons. Finally, we divide that number over the total number $m$ of observations from the validation set. This **accuracy** metric (ACC) gives an estimate of the times that the logistic regression will correctly work on unseen data. \n",
    "\n",
    "$$ \n",
    "ACC = \\sum\\limits_{i=1}^m \\frac{(\\text{pred}^{(i)} == y_{val}^{(i)})}{m}\n",
    "$$\n",
    "\n",
    "So, if the accuracy is equal to 0.5, it means that 50 percent of the time, the model is expected to work well. For instance, if the $Y_{val}$ and prediction vectors for five observations look like the example below, we compare each of their values and determine whether they match or not. After that, we have the following vector with a single 0 in the third position where the prediction and the label disagree. Next, we sum the number of times that the predictions were right and divide that number by the total number of observations in the validation sets. In the example, we get an accuracy equal to 80 percent. \n",
    "\n",
    "$$\n",
    "Y_{val} = \\left [\n",
    "\\begin{matrix}\n",
    "0  \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{matrix} \n",
    "\\right ] \\ \\ \\ \\ \\ \\ \\text{pred} = \\left [\n",
    "\\begin{matrix}\n",
    "0  \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{matrix} \n",
    "\\right ] \\ \\ \\ \\ \\rightarrow \\ \\ \\ \\ \\ (Y_{val} == \\text{pred}) = \\left [\n",
    "\\begin{matrix}\n",
    "1  \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{matrix} \n",
    "\\right ] \\ \\ \\ \\ \\ \\rightarrow \\ \\ \\ \\ \\ ACC = \\frac{4}{5} = 0.8\n",
    "$$\n",
    "\n",
    "\n",
    "Congratulations on finishing the first week of this specialization. You learned many concepts this week. The first thing you learned is you learned how to preprocess a text. You learned how to extract features from that text. You learned how to use those extracted features and train a model using those. Then you learned how to test your model. In this week's programming exercise, you're going to get a chance to implement all these concepts that we spoke about. Feel free to go ahead and do the programming exercise. There's also an optional video at the end of this week which covers the intuition behind the cost function for logistic regression. If you don't want to watch that video, feel free to go to next week, where you will learn about a new classification algorithm known as naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def accuracy(y_pred, y):\n",
    "    acc = 1./len(y_pred) * np.sum(y_pred == y)\n",
    "    return acc\n",
    "    \n",
    "y_pred = np.array([1, 1, 1, 0, 0, 1, 0, 0, 0, 0])\n",
    "y = np.array([1, 1, 1, 0, 0, 1, 0, 0, 0, 1])\n",
    "print('Accuracy: {}'.format(accuracy(y_pred, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: Cost Function\n",
    "\n",
    "Now, we learn about the intuition behind the logistic regression cost function. Specifically, we understand why the cost function is designed that way and what happens when we predict the true label and when we predict the wrong label.\n",
    "\n",
    "Let's have a look now at the equation of the cost function. First, look at the left hand side of the equation where we find a sum over the variable $m$, which is just the number of training examples in training set. This indicates that we sum over the cost of each training example and perform the average. The minus sign ensures that the overall costs will always be a positive number. Inside the square brackets, the equation has two terms that are added together. To consider what each of these terms contribute to the cost function for each training example, let's have a look at each of them separately. \n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum\\limits_{i=1}^{m} [\\color{blue}{y^{(i)} \\log h(x^{(i)}, \\theta)} + \\color{red}{(1 - y^{(i)}) \\log (1 - h(x^{(i)}, \\theta))}]\n",
    "$$\n",
    "\n",
    "The term on the left (in blue) is the product of $y^{(i)}$, which is the label for each training example, by the log of the prediction, which is the logistic regression function applied to each training example (represented as $h(x^{(i)}, \\theta)$. Now, consider the case when the label is $y^{(i)}=0$. In this case, the function $h$ can return any value, and the entire term will be 0 because 0 times anything is just 0. What about the case when the label is 1? If the prediction is close to 1, then the log of the prediction will be close to 0, because the $\\log 1 = 0$, and the product will also be near 0. If the label is 1, and the prediction is close to 0, then this term blows up and approaches negative infinity. Intuitively, we can see that this is the relevant term in the cost function when the label is 1. When the prediction is close to the label value, the loss is small, and when the label and prediction disagree, the overall cost goes up.\n",
    "\n",
    "|$y^{(i)}$ | $h(x^{(i)}, \\theta)$ | result |\n",
    "| :------: | :-----------------: | :----- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;any&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;0   |\n",
    "|    1     |         0.99        |   ~0   |\n",
    "|    1     |         ~0          |  -inf  |\n",
    "\n",
    "Now consider the term on the right hand side (in red) of the cost function equation, in this case, if the label is 1, then the $(1 - y)$ term goes to 0. So any value returned by the logistic regression function will result in a 0 for the entire term, because again, 0 times anything is just 0. If the label is 0, and the logistic regression function returns a value close to 0, then the products in this term will again be close to 0. If on the other hand the label is 0 and the prediction is close to 1, then the log term will blow up and the overall term will approach to negative infinity.\n",
    "\n",
    "|$y^{(i)}$ | $h(x^{(i)}, \\theta)$ | result |\n",
    "| :------: | :-----------------: | :----- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;any &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;0   |\n",
    "|    0     |         0.01        |   ~0   |\n",
    "|    0     |         ~1          |  -inf  |\n",
    "\n",
    "From this exercise, we can see now that there is one term in the cost function that is relevant when the label is 0 (in red), and another that is relevant when the label is 1 (in blue). In each of these terms, we take the log of a value between 0 and 1, which will always return a negative number. So, the minus sign out front ensures that the overall cost will always be a positive number. \n",
    "\n",
    "Now, let's have a look at what the cost function looks like for each of the labels. First, we are going to look at the loss when the label is 1 (blue curve). In the plot below, the prediction value is on the horizontal axis and the cost associated with a single training example is on the vertical axis. In this case $J(\\theta)$ simplifies to just $-\\log h(x^{(i)}, \\theta)$. When the prediction is close to 1, the loss is close to 0. Because the prediction agrees well with the label. When the prediction is close to 0, the loss approaches infinity, because the prediction and the label disagree strongly. \n",
    "\n",
    "<img src=\"images/logistic_terms.svg\" width=\"60%\"/>\n",
    "\n",
    "The opposite is true when the label is 0. In this case $J(\\theta)$ reduces to just $-\\log (1 - h(x^{(i)}, \\theta))$. Now when the prediction is close to 0, the loss is also close to 0. When the prediction is close to 1, the loss approaches infinity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
