# Week 1: The K-Armed Bandit Problem

For the first week of this course, you will learn how to understand the exploration-exploitation trade-off in sequential decision-making, implement incremental algorithms for estimating action-values, and compare the strengths and weaknesses to different algorithms for exploration. For this weekâ€™s graded assessment, you will implement and test an epsilon-greedy agent.

## Key Concepts
- Define reward
- Understand the temporal nature of the bandit problem
- Define k-armed bandit
- Define action-values
- Define action-value estimation methods
- Define exploration and exploitation
- Select actions greedily using an action-value function
- Define online learning
- Understand a simple online sample-average action-value estimation method
- Define the general online update equatio
- Understand why we might use a constant stepsize in the case of non-stationarity
- Define epsilon-greedy
- Compare the short-term benefits of exploitation and the long-term benefits of exploration
- Understand optimistic initial values
- Describe the benefits of optimistic initial values for early exploration
- Explain the criticisms of optimistic initial values
- Describe the upper confidence bound action selection method
- Define optimism in the face of uncertainty

The Module 01 file is better visualized using [NBViewer](https://nbviewer.jupyter.org/github/rogergranada/MOOCs/blob/master/Coursera/University%20of%20Alberta/Fundamentals%20of%20Reinforcement%20Learning/Week%201/Module%2001.ipynb).
