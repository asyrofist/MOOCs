{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Fundamentals\n",
    "\n",
    "What's the difference between **supervised learning**, **unsupervised learning**, and **reinforcement learning**? \n",
    "\n",
    "In **supervised learning** we assume the learner has access to labeled examples giving the correct answer. In **reinforcement learning**, the *reward* gives the agent some idea of how good or bad its recent actions were. You can think of supervised learning as requiring a teacher that helps you by telling you the correct answer. A *reward* on the other hand, is like having someone who can identify what good behavior looks like but can't tell you exactly how to do it. \n",
    "\n",
    "**Unsupervised learning** is about extracting underlying structure in data, *i.e.*, it's about the data representation. It can be used to construct representations that make a supervised or **RL** system better. In fact, techniques from both **supervised learning** and **unsupervised learning** can be used within **RL** to aid generalization. In **RL**, we focus on the problem of learning while interacting with an ever changing world. We don't expect our agents to simply compute a good behavior and then execute that behavior in an open loop fashion. We expect our agents to get things wrong, to refine their understanding as they go. The world is not a static place.\n",
    "\n",
    "An agent that immediately integrates its most recent experience should do well especially compared with ones that attempt to perfectly memorize how the world works. The idea of learning online is extremely powerful and is a defining feature of **RL**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road Map of the Course\n",
    "\n",
    "This course will closely follow Sutton and Barto's new addition of **Reinforcement Learning: An Introduction**. The RL book adhere to a simple principle: introduce each idea in the simplest setting it arises. In this spirit, we begin our study with **Multi-Arm Bandit** problems, where we get our first taste of the complexities of incremental learning: **exploration** and **exploitation**. \n",
    "\n",
    "After that, we move onto **Markov Decision Processes** to broaden the class of problems we can solve with reinforcement learning methods, where we will learn about balancing short-term and long-term reward. We will introduce key ideas like policies and value functions using almost all RL systems. \n",
    "\n",
    "We conclude with classic planning methods called **Dynamic Programming**, which have been used in large industrial control problems and can compute optimal policies given a complete model of the world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of Reinforcement Learning: Learning Objectives\n",
    "\n",
    "### Module 00: Welcome to the Course\n",
    "- Understand the prerequisites, goals and roadmap for the course.\n",
    "\n",
    "### Module 01: The K-Armed Bandit Problem\n",
    "\n",
    "**- Lesson 1: The K-Armed Bandit Problem**\n",
    "- Define reward\n",
    "- Understand the temporal nature of the bandit problem\n",
    "- Define k-armed bandit\n",
    "- Define action-values\n",
    "\n",
    "**Lesson 2: What to Learn? Estimating Action Values**\n",
    "- Define action-value estimation methods\n",
    "- Define exploration and exploitation\n",
    "- Select actions greedily using an action-value function\n",
    "- Define online learning\n",
    "- Understand a simple online sample-average action-value estimation method\n",
    "- Define the general online update equation\n",
    "- Understand why we might use a constant stepsize in the case of non-stationarity\n",
    "\n",
    "**Lesson 3: Exploration vs. Exploitation Tradeoff**\n",
    "- Define epsilon-greedy\n",
    "- Compare the short-term benefits of exploitation and the long-term benefits of exploration\n",
    "- Understand optimistic initial values\n",
    "- Describe the benefits of optimistic initial values for early exploration\n",
    "- Explain the criticisms of optimistic initial values\n",
    "- Describe the upper confidence bound action selection method\n",
    "- Define optimism in the face of uncertainty\n",
    "\n",
    "### Module 02: Markov Decision Processes\n",
    "\n",
    "**Lesson 1: Introduction to Markov Decision Processes**\n",
    "- Understand Markov Decision Processes, or MDPs\n",
    "- Describe how the dynamics of an MDP are defined\n",
    "- Understand the graphical representation of a Markov Decision Process\n",
    "- Explain how many diverse processes can be written in terms of the MDP framework\n",
    "\n",
    "**Lesson 2: Goal of Reinforcement Learning**\n",
    "- Describe how rewards relate to the goal of an agent\n",
    "- Understand episodes and identify episodic tasks\n",
    "\n",
    "**Lesson 3: Continuing Tasks**\n",
    "- Formulate returns for continuing tasks using discounting\n",
    "- Describe how returns at successive time steps are related to each other\n",
    "- Understand when to formalize a task as episodic or continuing\n",
    "\n",
    "### Module 03: Values Functions & Bellman Equations\n",
    "\n",
    "**Lesson 1: Policies and Value Functions**\n",
    "- Recognize that a policy is a distribution over actions for each possible state\n",
    "- Describe the similarities and differences between stochastic and deterministic policies\n",
    "- Identify the characteristics of a well-defined policy\n",
    "- Generate examples of valid policies for a given MDP\n",
    "- Describe the roles of state-value and action-value functions in reinforcement learning\n",
    "- Describe the relationship between value functions and policies\n",
    "- Create examples of valid value functions for a given MDP\n",
    "\n",
    "**Lesson 2: Bellman Equations**\n",
    "- Derive the Bellman equation for state-value functions\n",
    "- Derive the Bellman equation for action-value functions\n",
    "- Understand how Bellman equations relate current and future values\n",
    "- Use the Bellman equations to compute value functions\n",
    "\n",
    "**Lesson 3: Optimality (Optimal Policies & Value Functions)**\n",
    "- Define an optimal policy\n",
    "- Understand how a policy can be at least as good as every other policy in every state\n",
    "- Identify an optimal policy for given MDPs\n",
    "- Derive the Bellman optimality equation for state-value functions\n",
    "- Derive the Bellman optimality equation for action-value functions\n",
    "- Understand how the Bellman optimality equations relate to the previously introduced Bellman equations\n",
    "- Understand the connection between the optimal value function and optimal policies\n",
    "- Verify the optimal value function for given MDPs\n",
    "\n",
    "### Module 04: Dynamic Programming\n",
    "\n",
    "**Lesson 1: Policy Evaluation (Prediction)**\n",
    "- Understand the distinction between policy evaluation and control\n",
    "- Explain the setting in which dynamic programming can be applied, as well as its limitations\n",
    "- Outline the iterative policy evaluation algorithm for estimating state values under a given policy\n",
    "- Apply iterative policy evaluation to compute value functions\n",
    "\n",
    "**Lesson 2: Policy Iteration (Control)**\n",
    "- Understand the policy improvement theorem\n",
    "- Use a value function for a policy to produce a better policy for a given MDP\n",
    "- Outline the policy iteration algorithm for finding the optimal policy\n",
    "- Understand “the dance of policy and value”\n",
    "- Apply policy iteration to compute optimal policies and optimal value functions\n",
    "\n",
    "**Lesson 3: Generalized Policy Iteration**\n",
    "- Understand the framework of generalized policy iteration\n",
    "- Outline value iteration, an important example of generalized policy iteration\n",
    "- Understand the distinction between synchronous and asynchronous dynamic programming methods\n",
    "- Describe brute force search as an alternative method for searching for an optimal policy\n",
    "- Describe Monte Carlo as an alternative method for learning a value function\n",
    "- Understand the advantage of Dynamic programming and “bootstrapping” over these alternative strategies for finding the optimal policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
