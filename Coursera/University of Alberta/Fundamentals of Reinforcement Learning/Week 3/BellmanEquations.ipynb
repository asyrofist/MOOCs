{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rich Sutton and Andy Barto: A brief History of RL\n",
    "\n",
    "Andrew and I woke Reinforcement Learning up because it would add fallings on collect, and we clarified what it was and how it's different from supervised learning. This is sort of what I was characterizing as the origin story of reinforcement learning. It was an obvious idea, Marvin Minsky knew it in 1960 or '59. It's so obvious that everyone knew it, but then it became overshadowed by supervised learning until, Eric Loft started knocking on people's doors and saying, \"Hey, this is something that's been neglected.\" And as real and as important and tasked us to figure it out. In fact, the very first neural network simulation on a digital computer by farmland Clark in 1954 was a reinforcement learning system. \n",
    "\n",
    "I think at the time, search for something that works and then you will remember, combining search and memory is the essence of reinforcing, then strangely had been wrong. Donald Mickey, talked about mentalization which is one, RL is a memorized search. You do some operation, and then you remember the results, and the next time you have to do it. You look it up instead of recomputing, and it saves a lot of time and so on. In a sense, our RL at its root is memorized context-sensitive search. Pople Stone at the end of one of his paper on the 50ths talks about interpolator, like using polynomials instead of Lookup table to look up something with generalization. That's what neural networks do, for example. We didn't invent memorization, but through a new use for it. I don't know if people were doing memorized search the way reinforcement learner's. Here he had this idea of a distributed approach. He also had the idea of what you call a generalized reinforcement, that one of these units could be reinforced by all kinds of signals and not just our binary signal. Without making goal-seeking systems out of ballsy proponents, and without having a generalized reinforcement, would have just have a specialized reward signal. \n",
    "\n",
    "> I think that's what reinforcement learning is... It is just focusing on a learning system that actually wants something that does trial and error and remembers it, and has to specialized reward signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation Derivation\n",
    "In everyday life, we learn a lot without getting explicit, positive, or negative feedback. Imagine for example, you are riding a bike and hit a rock that sends you off balance. Let's say you recover so you don't suffer any injury. You might learn to avoid rocks in the future and perhaps react more quickly if you do hit one. We recognize that the state of losing our balance is bad even without falling and hurting ourselves. In reinforce and learning a similar idea allows us to relate the value of the current state to the value of future states without waiting to observe all the future rewards. We use **Bellman equations** to formalize this connection between the value of a state and its possible successors. \n",
    "\n",
    "### Bellman equation for the state-value function\n",
    "\n",
    "First, let's talk about the Bellman equation for the **state-value function**. The Bellman equation for the state value function defines a relationship between the value of a state and the value of his possible successor states. To derive this relationship from the definitions of the state-value function and return, let's start by recalling that the state-value function is defined as the expected return starting from the state $s$. Recall that the return is defined as the discounted sum of future rewards. \n",
    "\n",
    "$$\n",
    "V_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi} [\\color{blue}{G_t} | S_t = s] \\hspace{100px} \\color{blue}{G_t = \\sum\\limits_{k=0}^{\\infty} \\gamma^k R_{t+k+1}}\n",
    "$$\n",
    "\n",
    "We saw previously that the return at time $t$, can be written recursively as the immediate reward plus the discounted return at time $t+1$. \n",
    "\n",
    "$$\n",
    "V_{\\pi}(s) = \\mathbb{E}_{\\pi} [\\color{blue}{R_{t+1} + \\gamma G_{t+1}} | S_t = s]\n",
    "$$\n",
    "\n",
    "Now, let's expand this expected return. First, we expand the expected return as a sum over possible action choices made by the agent. Second, we expand over possible rewards and next states condition on state $s$ and action $a$. We can break it down in this order because the action choice depends only on the current state, while the next state and reward depend only on the current state and action. The result is a weighted sum of terms consisting of immediate reward plus expected future returns from the next state $s'$. \n",
    "\n",
    "$$\n",
    "V_{\\pi}(s) = \\color{green}{\\sum\\limits_a \\pi(a|s) \\sum\\limits_{s'}\\sum\\limits_{r} p(s',r | s, a)} \\left [r + \\gamma \\color{blue}{\\mathbb{E}_{\\pi} [G_{t+1} | S_{t+1} = s']}\\right ]\n",
    "$$\n",
    "\n",
    "All we have done is explicitly write the expectation as it's defined, as a sum of possible outcomes weighted by the probability that they occur. Note that capital $R_{t+1}$ is a random variable, while the little $r$ represents each possible reward outcome. The expected return depends on states and rewards infinitely far into the future. We could recursively expand this equation as many times as we want, but it would only make the expression more complicated.\n",
    "\n",
    "$$\n",
    "V_{\\pi}(s) = \\color{green}{\\sum\\limits_a \\pi(a|s) \\sum\\limits_{s'}\\sum\\limits_{r} p(s',r | s, a)} \\left [r + \\gamma \\color{blue}{\\sum\\limits_{a'} \\pi(a'|s') \\sum\\limits_{s''}\\sum\\limits_{r'} p(s'',r' | s', a') } \\left [r' + \\gamma \\color{red}{\\mathbb{E}_{\\pi} [G_{t+2} | S_{t+2} = s'' }\\right ] \\right ]\n",
    "$$\n",
    "\n",
    "Instead, we can notice that this expected return is also the definition of the value function for state $s'$. The only difference is that the time index is $t+1$ instead of $t$. This is not an issue because neither the policy nor $p$ depends on time. Making this replacement, we get the **Bellman equation for the state-value function**. \n",
    "\n",
    "$$\n",
    "V_{\\pi}(s) = \\color{green}{\\sum\\limits_a \\pi(a|s) \\sum\\limits_{s'}\\sum\\limits_{r} p(s',r | s, a)} \\left [r + \\gamma \\color{blue}{V_{\\pi(s')}}\\right ]\n",
    "$$\n",
    "\n",
    "The magic of value functions is that we can use them as a stand-in for the average of an infinite number of possible futures. \n",
    "\n",
    "### Bellman equation for the action value function\n",
    "\n",
    "We can derive a similar equation for the action-value function. Recall the original equation for action-value function.\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) \\doteq \\mathbb{E}_{\\pi} [G_{t} | S_{t} = s, A_{t} = a]\n",
    "$$\n",
    "\n",
    "We create a recursive equation for the value of a state action pair in terms of its possible successors state action pairs. In this case, the equation does not begin with the policy selecting an action. This is because the action is already fixed as part of the state action pair. Instead, we skip directly to the dynamics function $p$ to select the immediate reward and next state $s'$. Again, we have a weighted sum over terms consisting of immediate reward plus expected future return given a specific next state $s'$. \n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) = \\color{green}{\\sum\\limits_{s}\\sum\\limits_{r} p(s',r | s, a)} \\left [r + \\gamma \\color{blue}{\\mathbb{E}_{\\pi} [G_{t+1} | S_{t+1} = s']}\\right ]\n",
    "$$\n",
    "\n",
    "However, unlike the Bellman equation for the state-value function, we can't stop here. We want to recursive equation for the value of one state action pair in terms of the next state action pair. At the moment, we have the expected return given only the next state. To change this, we can express the expected return from the next state as a sum of the agents possible action choices. In particular, we can change the expectation to be conditioned on both the next state and the next action and then sum over all possible actions. Each term is weighted by the probability under $\\pi$ of selecting $a'$ in the state $s'$. \n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) = \\color{green}{\\sum\\limits_{s}\\sum\\limits_{r} p(s',r | s, a)} \\left [r + \\gamma \\sum\\limits_{a'} \\pi(a', s') \\color{blue}{\\mathbb{E}_{\\pi} [G_{t+1} | S_{t+1} = s', A_{t+1} = a']}\\right ]\n",
    "$$\n",
    "\n",
    "Now, this expected return is the same as the definition of the action-value function for $s'$ and $a'$. Making this replacement, we get the Bellman equation for the action value function. \n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) = \\color{green}{\\sum\\limits_{s}\\sum\\limits_{r} p(s',r | s, a)} \\left [r + \\gamma \\sum\\limits_{a'} \\pi(a', s') \\color{blue}{q_{\\pi}(s', a')}\\right ]\n",
    "$$\n",
    "\n",
    "So, we derived the Bellman equations for state and action value functions. These equations provide relationships between the values of a state or state action pair and the possible next states or next state action pairs. The Bellman equations capture an important structure of the reinforcement learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Bellman Equations?\n",
    "\n",
    "To illustrate the power of Bellman equations, consider the small example that consists of just four states, labeled A, B, C and D on a grid. The action space consists of moving up, down, left and right. Actions which would move off the gri (as in the last example), instead keep the agent in place. \n",
    "\n",
    "<img src=\"images/grid_world.svg\" width=\"100%\" align=\"center\"/>\n",
    "\n",
    "In this example, the reward is `0` everywhere except for any time the agent lands in state `B`. If the agent lands in state `B`, it gets a reward of `+5`. This includes starting in state `B` and hitting a wall to remain there. Let's consider the uniform random policy, which moves in every direction 25% of the time. The discount factor gamma 0.7.\n",
    "\n",
    "<img src=\"images/bellman_example.svg\" width=\"45%\" align=\"center\"/>\n",
    "\n",
    "To define the value of each of these states `A`, `B`, `C` and `D` under this policy, recall that the value function is defined as the expected return under policy $\\pi$. This is an average over the return obtained by each sequence of actions an agent could possibly choose, infinitely, many possible features. \n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "V_{\\pi}(A) \\doteq \\mathbb{E}_{\\pi}[G_t|S_t = A] \\\\\n",
    "V_{\\pi}(B) \\doteq \\mathbb{E}_{\\pi}[G_t|S_t = B]\\\\\n",
    "V_{\\pi}(C) \\doteq \\mathbb{E}_{\\pi}[G_t|S_t = C]\\\\\n",
    "V_{\\pi}(D) \\doteq \\mathbb{E}_{\\pi}[G_t|S_t = D]\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Using the Bellman equation, we can write down an expression for the value of state `A` in terms of the sum of the four possible actions and the resulting possible successor states. \n",
    "\n",
    "$$\n",
    "V_{\\pi}(s) = \\color{green}{\\sum\\limits_a \\pi(a|s)} \\color{red}{\\sum\\limits_{s'}\\sum\\limits_{r}} p(s',r | s, a) \\left [\\color{red}{r + \\gamma V_{\\pi(s')}}\\right ]\n",
    "$$\n",
    "\n",
    "We can simplify the expression further in this case, because for each action there's only one possible associated next state and reward. That's the sum over $s'$ and $r$ reduces to a single value. Note that here $s'$ and $r$ do still depend on the selected action, and the current state $s$. However, to keep the notation short, we haven't this explicitly. \n",
    "\n",
    "$$\n",
    "V_{\\pi}(A) = \\color{green}{\\sum\\limits_a \\pi(a|A)}\\color{red}{(r + 0.7V_{\\pi}(s'))}\n",
    "$$\n",
    "\n",
    "If we go right from state `A`, we land in state `B`, and receive a reward of `+5`. This happens one quarter of the time under the random policy. If we go down, we land in state `C`, and receive no immediate reward. Again, this occurs one-quarter of the time. If you go either up or left, we will land back in state `A` again. Each of the actions, up and left, again, occur one-quarter of the time. Since they both land in state `A` and received no reward, we combine them into a single term with factor of $\\frac{1}{2}$. \n",
    "\n",
    "$$ V_{\\pi}(A) = \\color{green}{\\frac{1}{4}}\\color{red}{5+0.7V_{\\pi}(B)} + \\color{green}{\\frac{1}{4}}\\color{red}{0.7V_{\\pi}(C)} + \\color{green}{\\frac{1}{2}}\\color{red}{0.7V_{\\pi}(A)}\n",
    "$$\n",
    "\n",
    "Similar to the equation for `A`, we can write down a similar equation for each of the other states, `B`, `C`, and `D`. Thus, we have a system of for equations for four variables that we can solve by hand, or put it into an automatic equation solver.\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "V_{\\pi}(A) =& \\color{green}{\\frac{1}{4}}\\color{red}{5+0.7V_{\\pi}(B)} &+& \\color{green}{\\frac{1}{4}}\\color{red}{0.7V_{\\pi}(C)} &+& \\color{green}{\\frac{1}{2}}\\color{red}{0.7V_{\\pi}(A)} \\\\\n",
    "V_{\\pi}(B) =& \\color{green}{\\frac{1}{2}}\\color{red}{5+0.7V_{\\pi}(B)} &+& \\color{green}{\\frac{1}{4}}\\color{red}{0.7V_{\\pi}(A)} &+& \\color{green}{\\frac{1}{4}}\\color{red}{0.7V_{\\pi}(D)} \\\\\n",
    "V_{\\pi}(C) =& \\color{green}{\\frac{1}{4}}\\color{red}{0.7V_{\\pi}(A)}\\hspace{7mm} &+& \\color{green}{\\frac{1}{4}}\\color{red}{0.7V_{\\pi}(D)} &+& \\color{green}{\\frac{1}{2}}\\color{red}{0.7V_{\\pi}(C)} \\\\\n",
    "V_{\\pi}(D) =& \\color{green}{\\frac{1}{4}}\\color{red}{5+0.7V_{\\pi}(B)} &+& \\color{green}{\\frac{1}{4}}\\color{red}{0.7V_{\\pi}(C)} &+& \\color{green}{\\frac{1}{2}}\\color{red}{0.7V_{\\pi}(D)} \\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "The unique solution is shown below. \n",
    "\n",
    "$$\n",
    "V_{\\pi}(A) = 4.2 \\hspace{10mm}V_{\\pi}(B) = 6.1 \\hspace{10mm}V_{\\pi}(C) = 2.2 \\hspace{10mm}V_{\\pi}(D) = 4.2\n",
    "$$\n",
    "\n",
    "The important thing to note is that the Bellman equation reduced an unmanageable infinite sum over possible futures, to a simple linear algebra problem. Perhaps for this small problem, you can come up with other ways to work out the values of each of these states. However the Bellman equation provides a powerful general relationship for MDPs. \n",
    "\n",
    "In this case, we used the Bellman equation to directly write down a system of equations for the state values, and then some the system to find the values. This approach may be possible for MDPs of moderate size. \n",
    "\n",
    "<img src=\"images/bellman_four_equations.svg\" width=\"60%\" align=\"center\"/>\n",
    "\n",
    "However, in more complex problems, this won't always be practical. Consider the game of chess for example. We probably won't be able to even list all the possible states, there are around $10^{45}$ of them. Constructing and solving the resulting system of Bellman equations would be a whole other story. Humans can learn to play chess very well. However, this simple game represents a tiny fraction of human experience, and humans can learn to do many things. Our agents should be able to learn many things too. \n",
    "\n",
    "To sum up, without the Bellman equation, we might have to consider an infinite number of possible futures. The Bellman equations exploit the structure of the MDP formulation, to reduce this infinite sum to a system of linear equations. We can then potentially solve the Bellman equation directly to find the state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
