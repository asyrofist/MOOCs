{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation vs. Control\n",
    "\n",
    "**Policy evaluation** is the task of determining the value function for a specific policy. **Control** is the task of finding a policy to obtain as much reward as possible. In other words, finding a policy which maximizes the value function. Control is the ultimate goal of reinforcement learning. But the task of policy evaluation is usually a necessary first step. It's hard to improve our policy if we don't have a way to assess how good it is.\n",
    "\n",
    "**Dynamic programming algorithms** use the Bellman equations to define iterative algorithms for both policy evaluation and control. Before diving into the details of this approach, imagine someone hands you a policy and your job is to determine how good that policy is. **Policy evaluation** is the task of determining the state value function $v_{\\pi}$ for a particular policy $\\pi$ ($\\pi \\rightarrow v_{\\pi}$. Recall that the value of a state under a policy $\\pi$ is the expected return from that state if we act according to $\\pi$. The return ($G_t$) is itself a discounted sum of future rewards. \n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi} [\\color{blue}{G_t} | S_t = s] \\hspace{100px} \\color{blue}{G_t = \\sum\\limits_{k=0}^{\\infty} \\gamma^k R_{t+k+1}}\n",
    "$$\n",
    "\n",
    "We have seen how the Bellman equation reduces the problem of finding $v_{\\pi}$ to a system of linear equations, one equation for each state. \n",
    "\n",
    "$$\n",
    "v_{\\color{green}{\\pi}}(s) = \\sum\\limits_a \\color{green}{\\pi}(a|s) \\sum\\limits_{s'}\\sum\\limits_{r} p(s',r | s, a) \\left [ r + \\gamma v_{\\color{green}{\\pi}}(s') \\right ]\n",
    "$$\n",
    "\n",
    "So the problem of policy evaluation reduces to solving this system of linear equations. In principle, we could approach this task with a variety of methods from linear algebra.\n",
    "\n",
    "<img src=\"images/linear_system_solver.svg\" width=\"35%\" align=\"center\"/>\n",
    "\n",
    "In practice, the iterative solution methods of dynamic programming are more suitable for general MDPs. \n",
    "\n",
    "<img src=\"images/dynamic_programming.svg\" width=\"35%\" align=\"center\"/>\n",
    "\n",
    "Control is the task of improving a policy. Recall that a policy $\\pi_2$ is considered as good as or better than $\\pi_1$ in the image (a) below if the value under $\\pi_{2}$ is greater than or equal to the value under $\\pi_1$ in every state. We say $\\pi_2$ is strictly better than $pi_1$ if $\\pi_2$ is as good as or better than $\\pi_1$ and there's at least one state where the value under $\\pi_2$ is strictly greater than the value under $\\pi_1$. The goal of the control task is to modify a policy to produce a new one which is strictly better. Moreover, we can try to improve the policy repeatedly to obtain a sequence of better and better policies. When this is no longer possible, it means there is no policy which is strictly better than the current policy. And so the current policy must be equal to an optimal policy ($\\pi_*$), and we can consider the control task complete. \n",
    "\n",
    "<img src=\"images/control.svg\" width=\"70%\" align=\"center\"/>\n",
    "\n",
    "Imagine we had access to the dynamics of the environment ($p$). We will learn how we can use this knowledge to solve the tasks of policy evaluation and control. Even with access to these dynamics, we will need careful thought and clever algorithms to compute value functions and optimal policies. We will investigate a class of solution methods called dynamic programming for this purpose. Dynamic programming uses the various Bellman equations we have seen, along with knowledge of $p$, to work out value functions and optimal policies. Classical dynamic programming does not involve interaction with the environment at all. Instead, we use dynamic programming methods to compute value functions and optimal policies given a model of the MDP. Nonetheless, dynamic programming is very useful for understanding other reinforced learning algorithms. Most reinforced learning algorithms can be seen as an approximation to dynamic programming without the model. This connection is perhaps most striking in the temporal different space dynamic planning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation\n",
    "\n",
    "**Dynamic programming algorithms** are obtained by turning the Bellman equations into update rules. Now, we will introduce the first of these algorithms called iterative policy evaluation. Remember the Bellman equation gives us a recursive expression for $v_\\pi$. \n",
    "\n",
    "$$\n",
    "v_{\\color{green}{\\pi}}(s) = \\sum\\limits_a \\color{green}{\\pi}(a|s) \\sum\\limits_{s'}\\sum\\limits_{r} p(s',r | s, a) \\left [ r + \\gamma v_{\\color{green}{\\pi}}(s') \\right ]\n",
    "$$\n",
    "\n",
    "The idea of iterative policy evaluation is so simple that at first it might seem a bit silly. We take the Bellman equation and directly use it as an update rule. Now, instead of an equation which holds for the true value function, we have a procedure we can apply to iteratively refine our estimate of the value function. This will produce a sequence of better and better approximations to the value function. \n",
    "\n",
    "$$\n",
    "v_{\\color{red}{k+1}}(s) \\leftarrow \\sum\\limits_a \\pi(a|s) \\sum\\limits_{s'}\\sum\\limits_{r} p(s',r | s, a) \\left [ r + \\gamma v_{\\color{red}{k}}(s') \\right ]\n",
    "$$\n",
    "\n",
    "Let's see visually how this procedure works. We begin with an arbitrary initialization for our approximate value function, let's call this $v_0$. Each iteration then produces a better approximation by using the update rule shown above. Each iteration applies this updates to every state $s$ in the state space, which we call a *sweep*. Applying this update repeatedly leads to a better and better approximation to the state value function $v_pi$. \n",
    "\n",
    "<img src=\"images/vpi.gif\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "If this update leaves the value function approximation unchanged, that is, if $v_{k+1}$ equals $v_k$ for all states, then $v_k$ equals $v_\\pi$, and we have found the value function. This is because $v_\\pi$ is the unique solution to the Bellman equation. The only way the update could leave $v_k$ unchanged is if $v_k$ already obeys the Bellman equation. In fact, it can be proven that for any choice of $v_0$, $v_k$ will converge to $v_\\pi$ in the limit as $k$ approaches infinity. \n",
    "\n",
    "$$\n",
    "\\lim\\limits_{k\\rightarrow\\infty} v_k = v_\\pi \\ \\ \\ \\ \\ \\text{for any} \\  v_0\n",
    "$$\n",
    "\n",
    "To implement iterative policy evaluation, we store two arrays ($V$ and $V'$), each has one entry for every state. \n",
    "\n",
    "$$\n",
    "\\color{red}{V'}(s) = \\sum\\limits_a \\pi(a|s) \\sum\\limits_{s'}\\sum\\limits_{r} p(s',r | s, a) \\left [ r + \\gamma \\color{red}{V}(s') \\right ]\n",
    "$$\n",
    "\n",
    "One array, which we label $V$ stores the current approximate value function. Another array, $V'$, stores the updated values. By using two arrays, we can compute the new values from the old one state at a time without the old values being changed in the process. At the end of a full sweep, we can write all the new values into $V$; then we do the next iteration. \n",
    "\n",
    "<img src=\"images/full_sweep.svg\" width=\"90%\" align=\"center\"/>\n",
    "\n",
    "It is also possible to implement a version with only one array, in which case, some updates will themselves use new values instead of old. This single array version is still guaranteed to converge, and in fact, will usually converge faster. This is because it gets to use the updated values sooner. \n",
    "\n",
    "<img src=\"images/single_array_version.svg\" width=\"7%\" align=\"center\"/>\n",
    "\n",
    "For simplicity, we focus on the two array version. Let's look at how iterative policy evaluation works on a particular example. Consider the four-by-four grid world shown below. This is an episodic MDP with the terminal state located in the top left and bottom right corners. The terminal state is shown in two places, but formally it is the same state. The reward will be minus one for every transition. Since the problem is episodic, let's consider the undiscounted case of $\\gamma=1$. There are four possible actions in each state up, down, left, and right. Each action is deterministic. If the action would move the agent off the grid, it instead leaves the agent in the same state. \n",
    "\n",
    "<img src=\"images/grid_example.svg\" width=\"30%\" align=\"center\"/>\n",
    "\n",
    "Now, let's evaluate the uniform random policy, which selects each of the four actions one-quarter of the time. The value function represents the expected number of steps until termination from a given state. The order we sweep through the states is not important since we are using the two array version of the algorithm. Let's assume we sweep the states first from left to right, and then from top to bottom. We never update the value of the terminal state as it is defined to be zero. \n",
    "\n",
    "We initialize all the values in $V$ to zero. The initial value stored in $V'$ are irrelevant since they'll always be updated before they are used. We can now begin our first iteration with the update to state one (marked in red in the image below). \n",
    "\n",
    "<img src=\"images/grid_example_values.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "To compute the update, we have to sum over all actions. Consider the left action first, which has probability one-quarter ($\\pi(a|s)=0.25$) under the uniform random policy. The dynamics function $p$ is deterministic here so only the reward and value for one $s'$ contributes to the sum. The sum includes minus one for the reward ($r=-1$), and zero for the value of the terminal state ($V(s')=0$). Since we initialized all state values to zero, and the reward for each transition is minus one, the computation for all the other actions will look much the same. \n",
    "\n",
    "$$\n",
    "V'(s) = 0.25*(-1+0) + 0.25*(-1+0) + 0.25*(-1+0) + 0.25*(-1+0) = -1\n",
    "$$\n",
    "\n",
    "The result is that $V'$ of state one is set to minus one. \n",
    "\n",
    "<img src=\"images/grid_example_values_1.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "Next, we move to state two. \n",
    "\n",
    "<img src=\"images/grid_example_values_2.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "We first evaluate the term in the sum for the left action. Again the action probability is one-quarter, and in this case, the next state is state one. Although we have updated the value of state one already, the version of the algorithm we are running we'll use the old value stored in $V$. So the value for state one in the update is still zero. \n",
    "\n",
    "$$\n",
    "V'(s) = 0.25*(-1+0) + 0.25*(-1+0) + 0.25*(-1+0) + 0.25*(-1+0) = -1\n",
    "$$\n",
    "\n",
    "Again, all the other actions will look much the same. The result is that $V'$ of state two is also set to minus one. \n",
    "\n",
    "<img src=\"images/grid_example_values_3.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "In fact, since every state value is initialized to zero, every state's value will be set to minus one. \n",
    "\n",
    "<img src=\"images/grid_example_values_4.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "After completing this full sweep, we copy the updated values from $V'$ to $V$. \n",
    "\n",
    "<img src=\"images/grid_example_values_5.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "This has been only one sweep. Let's discuss now the full algorithm for iterative policy evaluation. Take any policy we want to evaluate, initialize two arrays $V$ and $V'$. We can initialize these however we like, but let's set them to zero. We just saw how one sweep of iterative policy evaluation works. Let's look at how we compute multiple sweeps, and determine how the algorithm stops. The outer loop continues until the change in the approximate value function becomes small. We track the largest update to this state value in a given iteration. Let's call this delta ($\\Delta$). The outer loop terminates when this maximum change is less than some user-specified constant called theta. As discussed before, once the approximate value function stops changing, we have converged to $v_\\pi$. Similarly, once the change in the approximate value function is very small, this means we are close to $v_\\pi$. \n",
    "\n",
    "---\n",
    "$\n",
    "\\text{Algorithm: Iterative Policy Evaluation for estimating}\\ V \\approx v_\\pi \\\\\n",
    "\\\\\n",
    "\\text{Input}\\ \\pi\\text{, the policy to be evaluated}\\\\\n",
    "V \\leftarrow \\vec{0},\\ V' \\leftarrow \\vec{0}\\\\\n",
    "\\text{Loop}\\\\\n",
    "\\ \\ \\ \\Delta \\leftarrow 0\\\\\n",
    "\\ \\ \\ \\text{Loop for each}\\ s \\in \\mathcal{S}:\\\\\n",
    "\\ \\ \\ \\ \\ \\ V'(s) \\leftarrow \\sum\\limits_a \\pi(a|s) \\sum\\limits_{s',r} p(s',r|s,a)[r+\\gamma V(s')]\\\\\n",
    "\\ \\ \\ \\ \\ \\ \\Delta \\leftarrow \\max(\\Delta, |V'(s) - V(s)|)\\\\\n",
    "\\ \\ \\ V \\leftarrow V'\\\\\n",
    "\\text{until}\\ \\Delta < \\theta\\ \\ \\ \\text{(a small positive number)}\\\\\n",
    "\\text{Output}\\ V \\approx v_\\pi\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "Let's pick up where we left off in our grid world example. We had just completed our first sweep. Let's use our value of $\\theta=0.001$ for the stopping parameter theta. The smaller the value we choose, the more accurate our final value estimate will be. We've already completed one iteration, and the maximum change in value was $\\Delta=1.0$. Since this is greater than $\\theta=0.001$, we carry on to the next iteration. After the second sweep, notice how the terminal state starts to influence the value of the nearest states first. \n",
    "\n",
    "<img src=\"images/grid_example_values_6.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "Let's run one more sweep. We see that now the influence of the terminal state has propagated further. \n",
    "\n",
    "<img src=\"images/grid_example_values_7.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "Let's run a few more sweeps to see what happens. We can start to see how the value of each state is related to its proximity to the terminal state. Let's keep running until our maximum delta is less than theta. Here is the result we eventually arrive at, our approximate value function has converged to the value function for the random policy, and we're done. \n",
    "\n",
    "<img src=\"images/grid_example_values_8.svg\" width=\"90%\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value function:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Delta at iteration 10: 0.730255126953125\n",
      "Delta at iteration 20: 0.4239568174816668\n",
      "Delta at iteration 30: 0.2454359065894245\n",
      "Delta at iteration 40: 0.14208195501679555\n",
      "Delta at iteration 50: 0.08225065631786421\n",
      "Delta at iteration 60: 0.047614563296345835\n",
      "Delta at iteration 70: 0.027563872874601714\n",
      "Delta at iteration 80: 0.015956611490231865\n",
      "Delta at iteration 90: 0.009237216098327394\n",
      "Delta at iteration 100: 0.005347386022364731\n",
      "Delta at iteration 110: 0.003095579552088168\n",
      "Delta at iteration 120: 0.0017920181417991898\n",
      "Delta at iteration 130: 0.0010373918571673357\n",
      "Final delta: 0.0009822066677322994\n",
      "Final value function:\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_neighbors(mat, x, y):\n",
    "    \"\"\" Return the neighboor values of the matrix.\n",
    "    \"\"\"\n",
    "    #left\n",
    "    xl, xr = (max(0, x-1), y), (min(x+1, mat.shape[0]-1), y)\n",
    "    yu, yd = (x, max(0, y-1)), (x, min(y+1, mat.shape[1]-1))\n",
    "    indexes = [xl, xr, yu, yd]\n",
    "    values = []\n",
    "    for i, j in sorted(indexes):\n",
    "        values.append(mat[i][j])\n",
    "    return values\n",
    "\n",
    "def iteration(V, Vp, terminal_states, pi, reward):\n",
    "    delta = 0\n",
    "    for i in range(V.shape[0]):\n",
    "        for j in range(V.shape[1]):\n",
    "            if (i, j) not in terminal_states:\n",
    "                neighbors = get_neighbors(V, i, j)\n",
    "                vp = 0\n",
    "                for n in neighbors:\n",
    "                    vp += pi * (reward + n)\n",
    "                Vp[i][j] = vp\n",
    "                current_delta = np.absolute(Vp[i][j] - V[i][j])\n",
    "                delta = max(delta, current_delta)\n",
    "    return Vp, delta\n",
    "\n",
    "def run_algorithm(V, Vp, terminal_states, policy=0.25, reward=-1, theta=0.001):\n",
    "    \"\"\" Perform iterative policy evaluation\n",
    "    \"\"\"\n",
    "    delta = float('inf')\n",
    "    nb_iteration = 0\n",
    "    while delta > theta:\n",
    "        nb_iteration += 1\n",
    "        Vp, delta = iteration(V, Vp, terminal_states, policy, reward)\n",
    "        V = Vp.copy()\n",
    "        if nb_iteration % 10 == 0:\n",
    "            print('Delta at iteration {}: {}'.format(nb_iteration, delta))\n",
    "    return V, delta\n",
    "        \n",
    "V = np.zeros((4,4))\n",
    "Vp = np.zeros((4,4))\n",
    "terminal_states = [(0,0), (3,3)]\n",
    "theta = 0.001\n",
    "policy = 0.25\n",
    "reward = -1\n",
    "\n",
    "print('Initial value function:')\n",
    "print(V)\n",
    "finalV, delta = run_algorithm(V, Vp, terminal_states, policy, reward, theta)\n",
    "print('Final delta: {}'.format(delta))\n",
    "print('Final value function:')\n",
    "print(np.round(finalV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "\n",
    "We just looked at how dynamic programming can be used to iteratively evaluate a policy. We hinted that this was the first step towards the control task, which the goal is to improve a policy. Previously, we showed that given $v_*$, we can find the optimal policy by choosing the Greedy action. \n",
    "\n",
    "$$\n",
    "\\pi_{*}(s) = \\mathop{\\mathrm{argmax}}\\limits_{a} \\sum\\limits_{s'}\\sum\\limits_{r} p(s', r| s, a)[r + \\gamma \\color{red}{v_{*}}(s')]\n",
    "$$\n",
    "\n",
    "The Greedy action maximizes the Bellman's optimality equation in each state. Imagine instead of the optimal value function, we select an action which is greedy with respect to the value function $v_\\pi$ of an arbitrary policy $\\pi$. \n",
    "\n",
    "$$\n",
    "\\color{red}{?} = \\mathop{\\mathrm{argmax}}\\limits_{a} \\sum\\limits_{s'}\\sum\\limits_{r} p(s', r| s, a)[r + \\gamma v_{\\color{blue}{\\pi}}(s')]\n",
    "$$\n",
    "\n",
    "What can we say about this new policy? That it is greedy with respect to $v_\\pi$. The first thing to note is that this new policy must be different than $\\pi$. If this greedification doesn't change $\\pi$, then $\\pi$ was already greedy with respect to its own value function.\n",
    "\n",
    "$$\n",
    "\\color{blue}{\\pi}(s) = \\mathop{\\mathrm{argmax}}\\limits_{a} \\sum\\limits_{s'}\\sum\\limits_{r} p(s', r| s, a)[r + \\gamma v_{\\color{blue}{\\pi}}(s')]\\ \\ \\ \\text{for all}\\ s \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "This is just another way of saying that **$v_\\pi$ obeys the Bellman's optimality equation**. In which case, $\\pi$ is already optimal. In fact, the new policy obtained in this way must be a strict improvement on $\\pi$, unless $\\pi$ was already optimal. This is a consequence of a general result called the **policy improvement theorem**. \n",
    "\n",
    "### Policy improvement theorem\n",
    "\n",
    "Recall the definition of $q_\\pi$ that tells you the value of a state if you take action $a$, and then follow policy $\\pi$. \n",
    "\n",
    "$$\n",
    "q_{\\color{blue}{\\pi}}\\left (s, \\color{purple}{\\pi'}(s) \\right ) \\ge q_{\\color{blue}{\\pi}}\\left (s, \\color{blue}{\\pi}(s)\\right )\\ \\ \\ \\text{for all}\\ s \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "Imagine we take action $a$ according to $\\pi'$, and then follow policy $\\pi$. If this action has higher value than the action under $\\pi$, then $\\pi'$ must be better. The policy improvement theorem formalizes this idea. Policy $\\pi'$ is at least as good as $\\pi$ if in each state, the value of the action selected by $\\pi'$ is greater than or equal to the value of the action selected by $\\pi$. \n",
    "\n",
    "$$\n",
    "q_{\\color{blue}{\\pi}}\\left (s, \\color{purple}{\\pi'}(s) \\right ) \\ge q_{\\color{blue}{\\pi}}\\left (s, \\color{blue}{\\pi}(s)\\right )\\ \\ \\ \\text{for all}\\ s \\in \\mathcal{S}\\ \\ \\ \\rightarrow\\ \\ \\ \\color{purple}{\\pi'} \\ge \\color{blue}{\\pi}\n",
    "$$\n",
    "\n",
    "Policy $\\pi'$ is strictly better if the value is strictly greater and at least one state. \n",
    "\n",
    "$$\n",
    "q_{\\color{blue}{\\pi}}\\left (s, \\color{purple}{\\pi'}(s) \\right ) \\ge q_{\\color{blue}{\\pi}}\\left (s, \\color{blue}{\\pi}(s)\\right )\\ \\ \\ \\text{for all}\\ s \\in \\mathcal{S}\\ \\ \\ \\rightarrow\\ \\ \\ \\color{purple}{\\pi'} > \\color{blue}{\\pi}\n",
    "$$\n",
    "\n",
    "Let's see how this works on the four-by-four grid rolled we use previously. The final value function we found is shown on the left in the image below. Remember that this is the value function for the uniform random policy. Using a greedy policy $\\pi$, in each state, we need to select the action that leads to the next state with the highest value. In this case, the value that is least negative. The value of $\\pi'$ is quite different from the uniform random policy we started with. Know that the value shown on the right in the image below do not correspond to the values for $\\pi'$. \n",
    "\n",
    "<img src=\"images/policy_prime.svg\" width=\"60%\" align=\"center\"/>\n",
    "\n",
    "The new policy is guaranteed to be an improvement on the uniform random policy we started with according to the policy improvement theorem. In fact, if you look more closely at the new policy, we can see that it is in fact optimal. In every state, the chosen actions lie on the shortest path to the terminal state. Remember, the value function we started with was not the optimal value function, and yet the greedy policy with respect to $v_\\pi$ is optimal. More generally, the policy improvement theorem only guarantees that the new policy is an improvement on the original. We cannot always expect to find the optimal policy so easily. It is important to note the policy improvement theorem tells us that greedified $\\pi$ policy is a strict improvement, unless the original policy was already optimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script uses the same classes from the Module 04. \n",
    "It was removed the first two lines from _v_star() since\n",
    "they are designed for the specific problem of Module 04.\n",
    "From utils.py, we add the size of the figure to the plot, \n",
    "so the output plot is adjusted to the current matrix.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "class Board(object):\n",
    "    def __init__(self, values):\n",
    "        self.values = values\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self.values)):\n",
    "            for j in range(len(self.values[i])):\n",
    "                d = self._v_star(i, j)\n",
    "                r = self._rewards(i, j)\n",
    "                yield i, j, self.values[i][j], d, r\n",
    "                \n",
    "    def _v_star(self, x, y):\n",
    "        #if x == 0 and (y == 1 or y == 3):\n",
    "        #    return [16, 16, 16, 16] # l, u, r, d\n",
    "        dpos = [float('-inf'), float('-inf'), float('-inf'), float('-inf')]\n",
    "        if x > 0: dpos[1] = self.values[x-1][y]\n",
    "        if x < len(self.values)-1: dpos[3] = self.values[x+1][y]\n",
    "        if y > 0: dpos[0] = self.values[x][y-1]\n",
    "        if y < len(self.values[x])-1: dpos[2] = self.values[x][y+1]\n",
    "        return dpos\n",
    "    \n",
    "    def _rewards(self, x, y):\n",
    "        if x == 0 and y == 1:\n",
    "            return [10, 10, 10, 10]\n",
    "        elif  x == 0 and y == 3:\n",
    "            return [5, 5, 5, 5]\n",
    "        drew = [-1, -1, -1, -1]\n",
    "        if x > 0: drew[1] = 0\n",
    "        if x < len(self.values)-1: drew[3] = 0\n",
    "        if y > 0: drew[0] = 0\n",
    "        if y < len(self.values[x])-1: drew[2] = 0\n",
    "        return drew\n",
    "        \n",
    "           \n",
    "class Policy(object):\n",
    "    def __init__(self, board, gamma):\n",
    "        self.board = board\n",
    "        self.gamma = gamma\n",
    "        self.pi = np.zeros(np.array(self.board.values).shape)\n",
    "        \n",
    "    def run(self):\n",
    "        # r+gamma*v_sprime\n",
    "        for arr in self.board:\n",
    "            i, j, local_value, values, rewards = arr\n",
    "            st_values =  np.array(rewards) + gamma * np.array(values)\n",
    "            maxargs = np.argwhere(st_values == np.amax(st_values))\n",
    "            self.pi[i][j] = utils.encode_to_arrow(maxargs.flatten())\n",
    "        return self.pi            \n",
    "\n",
    "# Load the resulting matrix V from Iterative Policy Evaluation\n",
    "V = np.array([\n",
    "    [ 0.        , -13.98945772, -19.98437823, -21.98251832],\n",
    "    [-13.98945772, -17.98623815, -19.98448273, -19.98437823],\n",
    "    [-19.98437823, -19.98448273, -17.98623815, -13.98945772],\n",
    "    [-21.98251832, -19.98437823, -13.98945772,   0.        ]]\n",
    ")\n",
    "\n",
    "gamma = 0.9\n",
    "board = Board(V)\n",
    "pi = Policy(board, gamma)\n",
    "arrows = pi.run()\n",
    "utils.plot_arrows(arrows, figsize=V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "We just learned how the value function computed for a given policy can be used to find a better policy. Recall the policy improvement theorem. It tells us that we can construct a strictly better policy by acting greedily with respect to the value function of a given policy, unless the given policy was already optimal. \n",
    "\n",
    "$$\n",
    "\\color{blue}{\\pi'}(s) = \\mathop{\\mathrm{argmax}}\\limits_{a} \\sum\\limits_{s'}\\sum\\limits_{r} p(s', r| s, a)[r + \\gamma v_{\\color{blue}{\\pi}}(s')]\\ \\ \\ \\text{for all}\\ s \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "Let's say we begin with the policy $\\pi_1$. We can evaluate $\\pi_1$ using iterative policy evaluation to obtain the state value, $v_{\\pi_1}$. We call this the **evaluation** step. Using the results of the policy improvement theorem, we can then greedify with respect to $v_{\\pi_1}$ to obtain a better policy, $\\pi_2$. We call this the **improvement** step. We can then compute $v_{\\pi_2}$ and use it to obtain an even better policy, $\\pi_3$. \n",
    "\n",
    "$$\n",
    "\\pi_1\\ \\ \\ \\overset{E}{\\rightarrow}\\ \\ \\ v_{\\pi_1}\\ \\ \\ \\overset{I}{\\rightarrow}\\ \\ \\ \\pi_2\\ \\ \\ \\overset{E}{\\rightarrow}\\ \\ \\ v_{\\pi_2}\\ \\ \\ \\overset{I}{\\rightarrow}\\ \\ \\ \\pi_3\\ \\ \\ \\overset{E}{\\rightarrow}\\ \\ \\ \\ldots\n",
    "$$\n",
    "\n",
    "This gives us a sequence of better policies. Each policy is guaranteed to be an improvement on the last unless the last policy was already optimal. So when we complete an iteration, and the policy remains unchanged, we know we have found the optimal policy. At that point, we can terminate the algorithm. \n",
    "\n",
    "$$\n",
    "\\pi_1\\ \\ \\ \\overset{E}{\\rightarrow}\\ \\ \\ v_{\\pi_1}\\ \\ \\ \\overset{I}{\\rightarrow}\\ \\ \\ \\pi_2\\ \\ \\ \\overset{E}{\\rightarrow}\\ \\ \\ v_{\\pi_2}\\ \\ \\ \\overset{I}{\\rightarrow}\\ \\ \\ \\pi_3\\ \\ \\ \\overset{E}{\\rightarrow}\\ \\ \\ \\ldots\\ \\ \\ \\overset{I}{\\rightarrow}\\ \\ \\ \\pi_*\\ \\ \\ v_{\\pi_*}\\ \\ \\ \\overset{I}{\\rightarrow}\\ \\ \\ \\pi_*\n",
    "$$\n",
    "\n",
    "Each policy generated in this way is **deterministic**. There are finite number of deterministic policies, so this iterative improvement must eventually reach an optimal policy. This method of finding an optimal policy is called **policy iteration**. \n",
    "\n",
    "**Policy iteration** consists of two distinct steps repeated over and over, evaluation and improvement. We first evaluate our current policy, $\\pi_1$, which gives us a new value function that accurately reflects the value of $\\pi_1$. The improvement step then uses $v_{\\pi_1}$ to produce a greedy policy $\\pi_2$. At this point, $\\pi_2$ is greedy with respect to the value function of $\\pi_1$, but $v_{\\pi_1}$ no longer accurately reflects the value of $\\pi_2$. The next step evaluation makes our value function accurate with respect to the policy $\\pi_2$. Once we do this, our policy is once again not greedy. \n",
    "\n",
    "<img src=\"images/policy_iteration.svg\" width=\"30%\" align=\"center\"/>\n",
    "\n",
    "This dance of policy and value proceeds back and forth, until we reach the only policy, which is greedy with respect to it's own value function, the optimal policy. At this point, and only at this point, the policy is greedy and the value function is accurate. We can visualize this dance as bouncing back and forth between one line, where the value function is accurate, and another where the policy is greedy. These two lines intersect only at the optimal policy and value function. \n",
    "\n",
    "<img src=\"images/policy_dance.svg\" width=\"35%\" align=\"center\"/>\n",
    "\n",
    "Policy iteration always makes progress towards the intersection by projecting first onto the line $v = v_\\pi$, and then onto the line where $\\pi$ is greedy with respect to $v$. Of course, the real geometry of the space of policies and value functions is more complicated, but the same intuition holds. \n",
    "\n",
    "Here's what this procedure looks like in pseudocode. We initialize $v$ and $\\pi$ in any way we like for each state $s$. Next, we call iterative policy evaluation to make $V$ reflect the value of $\\pi$. This is the algorithm we learned earlier in this module. Then, in each state, we set $\\pi$ to select the maximizing action under the value function. If this procedure changes the selected action in any state, we note that the policy is still changing, and set policy stable to force. After completing step 3, we check if the policy is stable. If not, we carry on and evaluate the new policy. \n",
    "\n",
    "---\n",
    "$\n",
    "\\text{Algorithm: Policy Iteration (using iterative policy evaluation) for estimating}\\ \\pi \\approx \\pi_* \\\\\n",
    "$\n",
    "\n",
    "**1. Initialization**\n",
    "\n",
    "$\n",
    "V(s) \\in \\mathbb{R}\\ \\text{and}\\ \\pi(s) \\in \\mathcal{A}(s)\\ \\text{arbitrarily for all}\\ s \\in \\mathcal{S} \\\\\n",
    "$\n",
    "\n",
    "**2. Policy Evaluation**\n",
    "\n",
    "$\n",
    "\\text{2. Policy Evaluation} \\\\\n",
    "\\text{Loop}\\\\\n",
    "\\ \\ \\ \\Delta \\leftarrow 0\\\\\n",
    "\\ \\ \\ \\text{Loop for each}\\ s \\in \\mathcal{S}:\\\\\n",
    "\\ \\ \\ \\ \\ \\ v \\leftarrow V(s)\n",
    "\\ \\ \\ \\ \\ \\ V'(s) \\leftarrow \\sum\\limits_{s',r} p(s',r|s,\\pi(s))[r+\\gamma V(s')]\\\\\n",
    "\\ \\ \\ \\ \\ \\ \\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)\\\\\n",
    "\\text{until}\\ \\Delta < \\theta\\ \\ \\ \\text{(a small positive number determining the accuracy estimation)}\\\\\n",
    "$\n",
    "\n",
    "**3. Policy Improvement**\n",
    "\n",
    "$\n",
    "\\text{policy-stable} \\leftarrow True \\\\\n",
    "\\text{For each}\\ s \\in mathcal{S}: \\\\\n",
    "\\ \\ \\ \\text{old-action} \\leftarrow \\pi(s) \\\\\n",
    "\\ \\ \\ \\pi(s) \\leftarrow \\text{argmax}_a \\sum\\limits_{s',r} p(s',r|s,a) [r + \\gamma V(s')] \\\\\n",
    "\\ \\ \\ \\text{If old-action}\\ \\neq \\pi(s),\\ \\text{then policy-stable}\\ \\leftarrow False \\\\\n",
    "\\text{If policy-stable, then stop and return}\\ V \\approx v_*\\ \\text{and}\\ \\pi \\approx \\pi_*;\\ \\text{else go to}\\ 2\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "Let's look at how this works on a simple problem to build some intuition. Remember the four-by-four grid ruled example we used to demonstrate iterative policy evaluation. Previously, we showed that by evaluating the random policy, and greedifying just once, we could find the optimal policy. This is not a very interesting case for policy iteration. Let's modify this problem a little bit to make the control task a bit harder. First, let's remove one of the terminal states so that there's only one way to end the episode (represented as the gray cell on the top-left corner in image below). Previously, each state admitted a reward of `-1`. Instead, let's add some especially bad states. These bad states are marked as blue cells in the image below. Transitioning into them gives a reward of `-10`. The optimal policy should follow the winding low cost path in white to the terminal state. This additional complexity means that policy iteration takes several iterations to discover the path. \n",
    "\n",
    "<img src=\"images/example_iterative_policy.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "Let's see how this plays out in the image below. First, we initialize a policy and value function. As before, we choose the uniform random policy, and set the value estimate to zero for all states. The first step is to use iterative policy evaluation to evaluate the uniform random policy. Since you've seen how this works before, let's skip straight to the result. The values are quite negative everywhere, those slightly less so in states near the goal. Next we perform the improvement step. You've seen how greedification works before. So again, let's skip to the result. Notice that near the terminal state, the policy correctly follows the low cost path toward the terminal state. In the states in the bottom row however, the policy instead takes them more direct, but lower value path through the bad states. \n",
    "\n",
    "<img src=\"images/example_evaluation_improvement.svg\" width=\"70%\" align=\"center\"/>\n",
    "\n",
    "Let's evaluate this new policy. Notice how after just one improvement, the values are starting to look much more reasonable, but we aren't finished yet. \n",
    "\n",
    "<img src=\"images/evaluation_improvement_1.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "Let's greedify again. Remember, the policy improvement theorem tells us that this new policy is an improvement on the last one, unless we have already reached the optimum. Specifically, the bottom-right state now goes straight up along the low cost path. \n",
    "\n",
    "<img src=\"images/evaluation_improvement_2.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "One more step of policy evaluation reflects this change. The value of the bottom right state goes from `-15` to just `-6`. \n",
    "\n",
    "<img src=\"images/evaluation_improvement_3.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "Let's keep going until we reach the optimal policy. One more step of improvement improves the action selection and yet another state. \n",
    "\n",
    "<img src=\"images/evaluation_improvement_4.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "The next step of policy evaluation reflects this change. \n",
    "\n",
    "<img src=\"images/evaluation_improvement_5.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "Improve again, and evaluate, and improve. \n",
    "\n",
    "<img src=\"images/evaluation_improvement_6.svg\" width=\"40%\" align=\"center\"/>\n",
    "<img src=\"images/evaluation_improvement_7.svg\" width=\"40%\" align=\"center\"/>\n",
    "<img src=\"images/evaluation_improvement_8.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "Now, we can see that the policy has reached the optimum, and follows a low cost path avoiding the blue states. Evaluating one more time gives us the optimal value function. \n",
    "\n",
    "<img src=\"images/evaluation_improvement_9.svg\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "If we try to greedify again, the policy remains unchanged. This tells us that policy iteration is complete, and the optimal policy has been found. This example shows the power of policy iteration, in that it guarantees we can follow a sequence of increasingly better policies until we reach an optimal policy. Policy iteration cuts through the search space, which is key when the optimal policy is not straightforward, in this case literally. The same complexity will come up and problems we really care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flexibility of the Policy Iteration Framework\n",
    "\n",
    "So far, we've presented policy iteration as a fairly rigid procedure. We alternate between evaluating the current policy and greedify to improve the policy. The framework of generalized policy iteration allows much more freedom than this while maintaining our optimality guarantees. Recall the dance of policy and value, as shown in (a) in the image below. The policy iteration algorithm runs each step all the way to completion. Intuitively, we can imagine relaxing this. Imagine instead, we follow a trajectory like the one shown in (b) in the image below. \n",
    "\n",
    "<img src=\"images/policy_dance_real.svg\" width=\"60%\" align=\"center\"/>\n",
    "\n",
    "Each evaluation step brings our estimate a little closer to the value of the current policy but not all the way. Each policy improvement step makes our policy a little more greedy, but not totally greedy. Intuitively, this process should still make progress towards the optimal policy and value function. In fact, the theory tells us the same thing. \n",
    "\n",
    "We will use the term **Generalized Policy Iteration** to refer to all the ways we can interleave policy evaluation and policy improvement. This brings us to our first generalized policy iteration algorithm, called **Value Iteration**. In value iteration, we still sweep over all the states and greedify with respect to the current value function. However, we do not run policy evaluation to completion. We perform just one sweep over all the states. After that, we greedify again. We can write this as an update rule which applies directly to the state value function. \n",
    "\n",
    "---\n",
    "**Algorithm parameter**: a small threshold $\\theta > 0$ determining accuracy of estimation.<br>\n",
    "Initialize $V(s)$, for all $s \\in \\mathcal{S}^+$, arbitrarily except that $V($terminal$) = 0$.\n",
    "\n",
    "$\n",
    "\\text{Loop:} \\\\\n",
    "\\ \\ \\ \\Delta \\leftarrow 0 \\\\\n",
    "\\ \\ \\ \\text{Loop for each}\\ s \\in \\mathcal{S}: \\\\\n",
    "\\ \\ \\ \\ \\ \\ v \\leftarrow V(s) \\\\\n",
    "\\ \\ \\ \\ \\ \\ V(s) \\leftarrow \\max\\limits_a \\sum\\limits_{s',r} p(s',r|s, a) [r + \\gamma V(s')] \\\\\n",
    "\\ \\ \\ \\ \\ \\ \\Delta \\leftarrow \\max (\\Delta, |v - V(s)|) \\\\\n",
    "\\text{until}\\ \\Delta < \\theta\n",
    "$\n",
    "\n",
    "Output a deterministic policy $\\pi \\approx \\pi_*$, such that\n",
    "\n",
    "$\n",
    "\\pi(s) = \\text{argmax}_a \\sum\\limits_{s',r} p(s',r|s,a)[r + \\gamma V(s')]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "The update does not reference any specific policy, hence the name value iteration. The full algorithm looks very similar to iterative policy evaluation. Instead of updating the value according to a fixed falsey, we update using the action that maximizes the current value estimate. Value iteration still converges to $v_*$ in the limit. We can recover the optimal policy from the optimal value function by taking the argmax. In practice, we need to specify a termination condition because we can't wait forever. We will use the same condition we use for policy evaluation. We simply terminate when the maximum change in the value function over a full sweep is less than some small value $\\theta$. \n",
    "\n",
    "Value iteration sweeps the entire state space on each iteration just like policy iteration. Methods that perform systematic sweeps like the one show on the left in the image below are called synchronous. This can be problematic if the statespace is large. Every sweep could take a very long time. Asynchronous dynamic programming algorithms update the values of states in any order as shown on the right in the image below. They do not perform systematic sweeps, they might update a given state many times before another is updated even once. \n",
    "\n",
    "<img src=\"images/asynchronous.gif\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "In order to guarantee convergence, asynchronous algorithms must continue to update the values of all states. For example, the algorithm cannot update the same three states forever ignoring all the others. This is not acceptable because the other states cannot be correct if they are never updated at all. Asynchronous algorithms can propagate value information quickly through selective updates. Sometimes this can be more efficient than a systematic sweep. For example, an asynchronous method can update the states near those that have recently changed value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency of Dynamic Programming\n",
    "\n",
    "**Iterative Policy Evaluation** is the dynamic programming solution to the prediction or policy evaluation problem. Let's look at a sample-based alternative for policy evaluation. The value of each state can be treated as a totally independent estimation problem. First, recall that the value is the expected return from a given state.\n",
    "\n",
    "$$\n",
    "V_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi} [\\color{blue}{G_t} | S_t = s] \\hspace{100px} \\color{blue}{G_t = \\sum\\limits_{k=0}^{\\infty} \\gamma^k R_{t+k+1}}\n",
    "$$\n",
    "\n",
    "The procedure is simple, first, we gather a large number of returns under $\\pi$ and take their average. This will eventually converge to the state value, this is called the **Monte Carlo method**.\n",
    "\n",
    "<img src=\"images/monte_carlo.svg\" width=\"50%\" align=\"center\"/>\n",
    "\n",
    "However, if we do it this way, we may need a large number of returns from each state. Each return depends on many random actions, selected by $\\pi$, as well as many random state transitions due to the dynamics of the MDP. \n",
    "\n",
    "<img src=\"images/value_function.svg\" width=\"50%\" align=\"center\"/>\n",
    "\n",
    "We could be dealing with a lot of randomness here, each return might be very different than the true state value. So we may need to average many returns before the estimate converges, and we have to do this for every single state. \n",
    "\n",
    "The key insight of dynamic programming is that we do not have to treat the evaluation of each state as a separate problem. We can use the other value estimates we have already worked so hard to compute.\n",
    "\n",
    "$$\n",
    "v_{k+1}(\\color{red}{s}) \\leftarrow \\sum\\limits_a \\pi(a|s) \\sum\\limits_{s'}\\sum\\limits_{r} p(s',r | s, a) \\left [ r + \\gamma v_{k}(\\color{blue}{s'}) \\right ]\n",
    "$$\n",
    "\n",
    "This process of using the value estimates of successor states to improve our current value estimate is known as **bootstrapping**. \n",
    "\n",
    "<img src=\"images/bootstrapping.svg\" width=\"25%\" align=\"center\"/>\n",
    "\n",
    "This can be much more efficient than a Monte Carlo method that estimates each value independently.\n",
    "\n",
    "**Policy Iteration** computes optimal policies, **brute-force search** is a possible alternative. This method simply evaluates every possible deterministic policy one at a time, we then pick the one with the highest value. There are a finite number of deterministic policies, and there always exists an optimal deterministic policy. So brute-force search will find the answer eventually, however, the number of deterministic policies can be huge. A deterministic policy consists of one action choice per state. So the total number of deterministic policies is exponential in the number of states ($|\\mathcal{A}|^{|\\mathcal{S}|}$). Even on a fairly simple problem, this number could be massive, this process could take a very long time. The **Policy Improvement Theorem** guarantees that policy iteration will find a sequence of better and better policies. This is a significant improvement over exhaustively trying each and every policy.\n",
    "\n",
    "So how efficient is dynamic programming compared to these naive alternatives? Well, policy iteration is guaranteed to find the optimal policy in time polynomial in the number of states ($|\\mathcal{S}|$) and actions ($|\\mathcal{A}|$). Thus, dynamic programming is exponentially faster than the brute-force search of the policy space.\n",
    "\n",
    "In practice, dynamic programming is usually much faster, even in this worst-case guarantee. For example, the original four-by-four GridWorld converged in just one step of policy iteration. When we made the problem harder by adding bad states, it still converged in just five iterations. It might also seem restrictive that we have to run policy evaluation to completion for each step of policy iteration (Brute-Force search takes $4^{16}$ policies). In practice, this is not so bad, with each iteration, the policy tends to change less and less.\n",
    "\n",
    "The policy evaluation step changes the value function less and thus the evaluation step typically terminates quickly. Generally, solving an MDP gets harder as the number of states grows. The **curse of dimensionality** says that the size of the state space grows exponentially as the number of state variable increases. A single agent moving around a GridWorld is fine. But what if we wanted to coordinate a transportation network of thousands of drivers moving between hundreds of locations? A raw enumeration of the possible states could lead to an exponential blow-up. Clearly, this would lead to problems if we try to sweep the states to perform policy iteration. In fact, this is not an issue with dynamic programming. This is a statement about the difficulty of the problems we are interested in tackling, various techniques for mitigating this curse exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warren Powell: Approximate Dynamic Programming for Fleet Management\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#cc0000'>**error**</font>\n",
    "<font color='#2A52BE'>**step size**</font> \n",
    "<img src=\"images/reward_weight.svg\" width=\"40%\" align=\"center\"/>\n",
    "<font color='#cc0000'>&#9660;</font>\n",
    "<font color='#00ff00'>&#9650;</font>\n",
    "&#9744; \n",
    "<div class=\"alert alert-block alert-info\"><b>Note:</b> Use this equation in questions (2) to (5)</div>\n",
    "<div class=\"alert alert-block alert-success\"><b>Success:</b> Green box.</div>\n",
    "<div class=\"alert alert-block alert-danger\"><b>Danger:</b> Red box.</div>\n",
    "<code style=\"background:yellow;color:black\">Highlighting.</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    .output_png {\n",
       "    display: table-cell;\n",
       "    text-align: center;\n",
       "    vertical-align: middle;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Centralize images\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../_styles/custom.css\", \"r\").read() #or edit path to custom.css\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
